{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1 = np.loadtxt(\"ts1.txt\")\n",
    "ts2 = np.loadtxt(\"ts2.txt\")\n",
    "ts3 = np.loadtxt(\"ts3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Operations.ST_LocalExtrema import ST_LocalExrema as STLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctypes import *\n",
    "import ctypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_file = \"/Users/joshua/Desktop/MS_shannon.so\"\n",
    "lib = CDLL(so_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.entropy.argtypes = [\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_int,\n",
    "    ctypes.c_int,\n",
    "    ctypes.c_int,\n",
    "    ctypes.POINTER(ctypes.c_double)\n",
    "]\n",
    "lib.entropy.restype = None  # void function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(data, bin_count, depth):\n",
    "    \"\"\"\n",
    "    Calculate Shannon Entropy using the C function.\n",
    "    \n",
    "    Args:\n",
    "    data (numpy.array): Input time series data\n",
    "    bin_count (int): Number of bins for encoding\n",
    "    depth (int): Depth of the encoding\n",
    "    \n",
    "    Returns:\n",
    "    float: Calculated Shannon Entropy\n",
    "    \"\"\"\n",
    "    data = np.ascontiguousarray(data, dtype=np.float64)\n",
    "    length = len(data)\n",
    "    result = ctypes.c_double()\n",
    "    \n",
    "    lib.entropy(data, length, bin_count, depth, ctypes.byref(result))\n",
    "    \n",
    "    return result.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.258990526199341"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shannon_entropy(ts3, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numBins = 10\n",
    "numBins = [numBins]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Operations.CO_Embed2_Shapes import CO_Embed2_Shapes as coes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Operations.CO_HistogramAMI import CO_HistogramAMI\n",
    "from Operations.CO_FirstCrossing import CO_FirstCrossing\n",
    "from Operations.IN_AutoMutualInfo import IN_AutoMutualInfo\n",
    "from Operations.CO_AutoCorr import CO_AutoCorr\n",
    "from PeripheryFunctions.BF_SignChange import BF_SignChange\n",
    "from PeripheryFunctions.BF_iszscored import BF_iszscored\n",
    "from scipy.optimize import curve_fit\n",
    "import warnings\n",
    "\n",
    "def CO_AddNoise(y, tau = 1, amiMethod = 'even', extraParam = None, randomSeed = None):\n",
    "    \"\"\"\n",
    "    CO_AddNoise: Changes in the automutual information with the addition of noise\n",
    "\n",
    "    Parameters:\n",
    "    y (array-like): The input time series (should be z-scored)\n",
    "    tau (int or str): The time delay for computing AMI (default: 1)\n",
    "    amiMethod (str): The method for computing AMI:\n",
    "                      'std1','std2','quantiles','even' for histogram-based estimation,\n",
    "                      'gaussian','kernel','kraskov1','kraskov2' for estimation using JIDT\n",
    "    extraParam: e.g., the number of bins input to CO_HistogramAMI, or parameter for IN_AutoMutualInfo\n",
    "    randomSeed (int): Settings for resetting the random seed for reproducible results\n",
    "\n",
    "    Returns:\n",
    "    dict: Statistics on the resulting set of automutual information estimates\n",
    "    \"\"\"\n",
    "\n",
    "    if not BF_iszscored(y):\n",
    "        warnings.warn(\"Input time series should be z-scored\")``\n",
    "    \n",
    "    # Set tau to minimum of autocorrelation function if 'ac' or 'tau'\n",
    "    if tau in ['ac', 'tau']:\n",
    "        tau = CO_FirstCrossing(y, 'ac', 0, 'discrete')\n",
    "    \n",
    "    # Generate noise\n",
    "    if randomSeed is not None:\n",
    "        np.random.seed(randomSeed)\n",
    "    noise = np.random.randn(len(y)) # generate uncorrelated additive noise\n",
    "\n",
    "    # Set up noise range\n",
    "    noiseRange = np.linspace(0, 3, 50) # compare properties across this noise range\n",
    "    numRepeats = len(noiseRange)\n",
    "\n",
    "    # Compute the automutual information across a range of noise levels\n",
    "    amis = np.zeros(numRepeats)\n",
    "    if amiMethod in ['std1', 'std2', 'quantiles', 'even']:\n",
    "        # histogram-based methods using my naive implementation in CO_Histogram\n",
    "        for i in range(numRepeats):\n",
    "            amis[i] = CO_HistogramAMI(y + noiseRange[i]*noise, tau, amiMethod, extraParam or 10) # use default numBins if None\n",
    "            if np.isnan(amis[i]):\n",
    "                raise ValueError('Error computing AMI: Time series too short (?)')\n",
    "    if amiMethod in ['gaussian','kernel','kraskov1','kraskov2']:\n",
    "        for i in range(numRepeats):\n",
    "            amis[i] = IN_AutoMutualInfo(y + noiseRange[i]*noise, tau, amiMethod, extraParam)\n",
    "            if np.isnan(amis[i]):\n",
    "                raise ValueError('Error computing AMI: Time series too short (?)')\n",
    "    \n",
    "    # Output statistics\n",
    "    out = {}\n",
    "    # Proportion decreases\n",
    "    out['pdec'] = np.sum(np.diff(amis) < 0) / (numRepeats - 1)\n",
    "\n",
    "    # Mean change in AMI\n",
    "    out['meanch'] = np.mean(np.diff(amis))\n",
    "\n",
    "    # Autocorrelation of AMIs\n",
    "    out['ac1'] = CO_AutoCorr(amis, 1, 'Fourier')[0]\n",
    "    out['ac2'] = CO_AutoCorr(amis, 2, 'Fourier')[0]\n",
    "\n",
    "    # Noise level required to reduce ami to proportion x of its initial value\n",
    "    firstUnderVals = [0.75, 0.50, 0.25]\n",
    "    for val in firstUnderVals:\n",
    "        out[f'firstUnder{val*100}'] = firstUnder_fn(val * amis[0], noiseRange, amis)\n",
    "\n",
    "    # AMI at actual noise levels: 0.5, 1, 1.5 and 2\n",
    "    noiseLevels = [0.5, 1, 1.5, 2]\n",
    "    for nlvl in noiseLevels:\n",
    "        out[f'ami_at_{nlvl*10}'] = amis[np.argmax(noiseRange >= nlvl)]\n",
    "\n",
    "    # Count number of times the AMI function crosses its mean\n",
    "    out['pcrossmean'] = np.sum(np.diff(np.sign(amis - np.mean(amis))) != 0) / (numRepeats - 1)\n",
    "\n",
    "    # Fit exponential decay\n",
    "    expFunc = lambda x, a, b : a * np.exp(b * x)\n",
    "    popt, pcov = curve_fit(expFunc, noiseRange, amis, p0=[amis[0], -1])\n",
    "    out['fitexpa'], out['fitexpb'] = popt\n",
    "    residuals = amis - expFunc(noiseRange, *popt)\n",
    "    ss_res = np.sum(residuals**2)\n",
    "    ss_tot = np.sum((amis - np.mean(amis))**2)\n",
    "    out['fitexpr2'] = 1 - (ss_res / ss_tot)\n",
    "    out['fitexpadjr2'] = 1 - (1-out['fitexpr2'])*(len(amis)-1)/(len(amis)-2-1)\n",
    "    out['fitexprmse'] = np.sqrt(np.mean(residuals**2))\n",
    "\n",
    "    # Fit linear function\n",
    "    p = np.polyfit(noiseRange, amis, 1)\n",
    "    out['fitlina'], out['fitlinb'] = p\n",
    "    lin_fit = np.polyval(p, noiseRange)\n",
    "    out['linfit_mse'] = np.mean((lin_fit - amis)**2)\n",
    "\n",
    "    return out\n",
    "\n",
    "# helper functions\n",
    "def firstUnder_fn(x, m, p):\n",
    "    \"\"\"\n",
    "    Find the value of m for the first time p goes under the threshold, x. \n",
    "    p and m vectors of the same length\n",
    "    \"\"\"\n",
    "    first_i = next((m_val for m_val, p_val in zip(m, p) if p_val < x), m[-1])\n",
    "    return first_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_intervals(elements, scales):\n",
    "    return np.array([int((elements / (1 << scale)) + 0.5) for scale in range(scales - 1, -1, -1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfa(x, intervals):\n",
    "    elements = len(x)\n",
    "    flucts = np.zeros(len(intervals))\n",
    "\n",
    "    for scale, interval in enumerate(intervals):\n",
    "        subdivs = int(np.ceil(elements / interval))\n",
    "        trend = np.zeros(elements)\n",
    "\n",
    "        for i in range(subdivs):\n",
    "            start = i * interval\n",
    "            end = start + interval\n",
    "            if end > elements:\n",
    "                trend[start:] = x[start:]\n",
    "                break\n",
    "            segment = x[start:end]\n",
    "            t = np.arange(interval)\n",
    "            coeffs = np.polyfit(t, segment, 1)\n",
    "            trend[start:end] = np.polyval(coeffs, t)\n",
    "        flucts[scale] = np.sqrt(np.mean((x - trend)**2))\n",
    "\n",
    "    return flucts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastdfa(x, intervals=None):\n",
    "    \"\"\"\n",
    "    Perform fast detrended fluctuation analysis on a nonstationary input signal.\n",
    "\n",
    "    Args:\n",
    "    x: Input signal (must be a 1D numpy array)\n",
    "    intervals: Optional list of sample interval widths at each scale\n",
    "\n",
    "    Returns:\n",
    "    intervals: List of sample interval widths at each scale\n",
    "    flucts: List of fluctuation amplitudes at each scale\n",
    "    \"\"\"\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(\"Input sequence must be a vector.\")\n",
    "    \n",
    "    elements = len(x)\n",
    "    \n",
    "    if intervals is None:\n",
    "        scales = int(np.log2(elements))\n",
    "        if (1 << (scales - 1)) > elements / 2.5:\n",
    "            scales -= 1\n",
    "        intervals = _calculate_intervals(elements, scales)\n",
    "    else:\n",
    "        if len(intervals) < 2:\n",
    "            raise ValueError(\"Number of intervals must be greater than one.\")\n",
    "        if np.any((intervals > elements) | (intervals < 3)):\n",
    "            raise ValueError(\"Invalid interval size: must be between size of sequence x and 3.\")\n",
    "    \n",
    "    y = np.cumsum(x)\n",
    "    flucts = dfa(y, intervals)\n",
    "    \n",
    "    return intervals, flucts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpts, ypts = fastdfa(ts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = np.polyfit(np.log10(xpts), np.log10(ypts), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.61481407, -0.93273621])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Operations.SC_fastdfa import SC_fastdfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.614814074638312"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SC_fastdfa(ts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _buffer(X, n, p=0, opt=None):\n",
    "    '''Mimic MATLAB routine to generate buffer array\n",
    "\n",
    "    MATLAB docs here: https://se.mathworks.com/help/signal/ref/buffer.html.\n",
    "    Taken from: https://stackoverflow.com/questions/38453249/does-numpy-have-a-function-equivalent-to-matlabs-buffer \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: ndarray\n",
    "        Signal array\n",
    "    n: int\n",
    "        Number of data segments\n",
    "    p: int\n",
    "        Number of values to overlap\n",
    "    opt: str\n",
    "        Initial condition options. default sets the first `p` values to zero,\n",
    "        while 'nodelay' begins filling the buffer immediately.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : (n,n) ndarray\n",
    "        Buffer array created from X\n",
    "    '''\n",
    "    import numpy as np\n",
    "\n",
    "    if opt not in [None, 'nodelay']:\n",
    "        raise ValueError('{} not implemented'.format(opt))\n",
    "\n",
    "    i = 0\n",
    "    first_iter = True\n",
    "    while i < len(X):\n",
    "        if first_iter:\n",
    "            if opt == 'nodelay':\n",
    "                # No zeros at array start\n",
    "                result = X[:n]\n",
    "                i = n\n",
    "            else:\n",
    "                # Start with `p` zeros\n",
    "                result = np.hstack([np.zeros(p), X[:n-p]])\n",
    "                i = n-p\n",
    "            # Make 2D array and pivot\n",
    "            result = np.expand_dims(result, axis=0).T\n",
    "            first_iter = False\n",
    "            continue\n",
    "\n",
    "        # Create next column, add `p` results from last col if given\n",
    "        col = X[i:i+(n-p)]\n",
    "        if p != 0:\n",
    "            col = np.hstack([result[:,-1][-p:], col])\n",
    "        i += n-p\n",
    "\n",
    "        # Append zeros if last row and not length `n`\n",
    "        if len(col) < n:\n",
    "            col = np.hstack([col, np.zeros(n-len(col))])\n",
    "\n",
    "        # Combine result with next row\n",
    "        result = np.hstack([result, np.expand_dims(col, axis=0).T])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Operations.CO_AutoCorr import CO_AutoCorr\n",
    "from warnings import warn\n",
    "from scipy.interpolate import CubicSpline\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def SC_FluctAnal(x, q = 2, wtf = 'rsrange', tauStep = 1, k = 1, lag = None, logInc = True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(x) # time series length\n",
    "\n",
    "    # Compute integrated sequence\n",
    "    if lag is None or lag == 1:\n",
    "        y = np.cumsum(x) # normal cumulative sum\n",
    "    else:\n",
    "        y = np.cumsum(x[::lag]) # if a lag is specified, do a decimation\n",
    "    \n",
    "    # Perform scaling over a range of tau, up to a fifth the time-series length\n",
    "    #-------------------------------------------------------------------------------\n",
    "    # Peng (1995) suggests 5:N/4 for DFA\n",
    "    # Caccia suggested from 10 to (N-1)/2...\n",
    "    #-------------------------------------------------------------------------------\n",
    "\n",
    "    if logInc:\n",
    "        # in this case tauStep is the number of points to compute\n",
    "        if tauStep == 1:\n",
    "            # handle the case where tauStep is 1, but we want to take the upper \n",
    "            # limit (MATLAB) rather than the lower (Python)\n",
    "            tauStep += 1\n",
    "            logRange = np.linspace(np.log(5), np.log(np.floor(N/2)), tauStep)[1] # take the second entry (upper limit)\n",
    "        else:\n",
    "            logRange = np.linspace(np.log(5), np.log(np.floor(N/2)), tauStep)\n",
    "        taur = np.unique(np.round(np.exp(logRange)).astype(int))\n",
    "    else:\n",
    "        taur = np.arange(5, int(np.floor(N/2)) + 1, tauStep)\n",
    "    ntau = len(taur) # % analyze the time series across this many timescales\n",
    "    #print(taur)\n",
    "    if ntau < 8: # fewer than 8 points\n",
    "        # time series is too short for analysing using this fluctuation analysis. \n",
    "        warn(f\"This time series (N = {N}) is too short to analyze using this fluctuation analysis.\")\n",
    "        out = np.NaN\n",
    "    \n",
    "    # 2) Compute the fluctuation function, F\n",
    "    F = np.zeros(ntau)\n",
    "    # each entry corresponds to a given scale, tau\n",
    "    for i in range(ntau):\n",
    "        # buffer the time series at the scale tau\n",
    "        tau = taur[i]\n",
    "        y_buff = _buffer(y, tau)\n",
    "        if y_buff.shape[1] > int(np.floor(N/tau)): # zero-padded, remove trailing set of pts...\n",
    "            y_buff = y_buff[:, :-1]\n",
    "\n",
    "        # analyzed length of time series (with trailing pts removed)\n",
    "        nn = y_buff.shape[1] * tau\n",
    "\n",
    "        if wtf == 'nothing':\n",
    "            y_dt = y_buff.reshape(nn, 1)\n",
    "        elif wtf == 'endptdiff':\n",
    "            # look at differences in end-points in each subsegment\n",
    "            y_dt = y_buff[-1, :] - y_buff[0, :]\n",
    "        elif wtf == 'range':\n",
    "            y_dt = np.ptp(y_buff, axis=0)\n",
    "        elif wtf == 'std':\n",
    "            y_dt = np.std(y_buff, ddof=1, axis=0)\n",
    "        elif wtf == 'iqr':\n",
    "            y_dt = np.percentile(y_buff, 75, method='hazen', axis=0) - np.percentile(y_buff, 25, method='hazen', axis=0)\n",
    "        elif wtf == 'dfa':\n",
    "            tt = np.arange(1, tau + 1)[:, np.newaxis]\n",
    "            for j in range(y_buff.shape[1]):\n",
    "                p = np.polyfit(tt.ravel(), y_buff[:, j], k)\n",
    "                y_buff[:, j] -= np.polyval(p, tt).ravel()\n",
    "            y_dt = y_buff.reshape(-1)\n",
    "        elif wtf == 'rsrange':\n",
    "            # Remove straight line first: Caccia et al. Physica A, 1997\n",
    "            # Straight line connects end points of each window:\n",
    "            b = y_buff[0, :]\n",
    "            m = y_buff[-1, :] - b\n",
    "            y_buff -= (np.linspace(0, 1, tau)[:, np.newaxis] * m + b)\n",
    "            y_dt = np.ptp(y_buff, axis=0)\n",
    "        elif wtf == 'rsrangefit':\n",
    "            # polynomial fit (order k) rather than endpoints fit: (~DFA)\n",
    "            tt = np.arange(1, tau + 1)[:, np.newaxis]\n",
    "            for j in range(y_buff.shape[1]):\n",
    "                p = np.polyfit(tt.ravel(), y_buff[:, j], k)\n",
    "                y_buff[:, j] -= np.polyval(p, tt).ravel()\n",
    "            y_dt = np.ptp(y_buff, axis=0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fluctuation analysis method '{wtf}\")\n",
    "        \n",
    "        F[i] = (np.mean(y_dt**q))**(1/q)\n",
    "\n",
    "    # Smooth unevenly-distributed points in log space\n",
    "    if logInc:\n",
    "        logtt = np.log(taur)\n",
    "        logFF = np.log(F)\n",
    "        numTimeScales = ntau\n",
    "    else:\n",
    "        # need to smooth the unevenly-distributed pts (using a spline)\n",
    "        logtaur = np.log(taur)\n",
    "        logF = np.log(F)\n",
    "        numTimeScales = 50 # number of sampling pts across the range\n",
    "        logtt = np.linspace(np.min(logtaur), np.max(logtaur), numTimeScales) # even sampling in tau\n",
    "        # equivalent to spline function in MATLAB\n",
    "        spl_fit = CubicSpline(logtaur, logF)\n",
    "        logFF = spl_fit(logtt)\n",
    "\n",
    "    # Linear fit the log-log plot: full range\n",
    "    out = {}\n",
    "    out = doRobustLinearFit(out, logtt, logFF, range(numTimeScales), '')\n",
    "    \n",
    "    \"\"\" \n",
    "    WE NEED SOME SORT OF AUTOMATIC DETECTION OF GRADIENT CHANGES/NUMBER\n",
    "    %% OF PIECEWISE LINEAR PIECES\n",
    "\n",
    "    ------------------------------------------------------------------------------\n",
    "    Try assuming two components (2 distinct scaling regimes)\n",
    "    ------------------------------------------------------------------------------\n",
    "    Move through, and fit a straight line to loglog before and after each point.\n",
    "    Find point with the minimum sum of squared errors\n",
    "\n",
    "    First spline interpolate to get an even sampling of the interval\n",
    "    (currently, in the log scale, there are relatively more at slower timescales)\n",
    "\n",
    "    Determine the errors\n",
    "    \"\"\"\n",
    "    sserr = np.full(numTimeScales, np.nan) # don't choose the end pts\n",
    "    minPoints = 6\n",
    "    for i in range(minPoints, (numTimeScales-minPoints)+1):\n",
    "        r1 = np.arange(i)\n",
    "        p1 = np.polyfit(logtt[r1], logFF[r1], 1) # first degree polynomial\n",
    "        r2 = np.arange(i-1, numTimeScales)\n",
    "        p2 = np.polyfit(logtt[r2], logFF[r2], 1)\n",
    "        sserr[i] = (np.linalg.norm(np.polyval(p1, logtt[r1]) - logFF[r1]) +\n",
    "                    np.linalg.norm(np.polyval(p2, logtt[r2]) - logFF[r2]))\n",
    "    \n",
    "    # breakPt is the point where it's best to fit a line before and another line after\n",
    "    breakPt = np.nanargmin(sserr)\n",
    "    r1 = np.arange(breakPt)\n",
    "    r2 = np.arange(breakPt-1, numTimeScales)\n",
    "\n",
    "    # Proportion of the domain of timescales corresponding to the first good linear fit\n",
    "    out['prop_r1'] = len(r1)/numTimeScales\n",
    "    out['logtausplit'] = logtt[breakPt-1]\n",
    "    out['ratsplitminerr'] = np.nanmin(sserr) / out['ssr']\n",
    "    out['meanssr'] = np.nanmean(sserr)\n",
    "    out['stdssr'] = np.nanstd(sserr, ddof=1)\n",
    "\n",
    "    # Check that at least 3 points are available\n",
    "    # Now we perform the robust linear fitting and get statistics on the two segments\n",
    "    # R1\n",
    "    out = doRobustLinearFit(out, logtt, logFF, r1, 'r1_')\n",
    "    # R2\n",
    "    out = doRobustLinearFit(out, logtt, logFF, r2, 'r2_')\n",
    "\n",
    "    if np.isnan(out['r1_alpha']) or np.isnan(out['r2_alpha']):\n",
    "        out['alpharat'] = np.nan\n",
    "    else:\n",
    "        out['alpharat'] = out['r1_alpha'] / out['r2_alpha']\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doRobustLinearFit(out, logtt, logFF, theRange, fieldName):\n",
    "    \"\"\"\n",
    "    Get robust linear fit statistics on scaling range\n",
    "    Adds fields to the output structure\n",
    "    \"\"\"\n",
    "    if len(theRange) < 8 or np.all(np.isnan(logFF[theRange])):\n",
    "        out[f'{fieldName}linfitint'] = np.nan\n",
    "        out[f'{fieldName}alpha'] = np.nan\n",
    "        out[f'{fieldName}se1'] = np.nan\n",
    "        out[f'{fieldName}se2'] = np.nan\n",
    "        out[f'{fieldName}ssr'] = np.nan\n",
    "        out[f'{fieldName}resac1'] = np.nan\n",
    "    else:\n",
    "        X = sm.add_constant(logtt[theRange])\n",
    "        model = sm.RLM(logFF[theRange], X, M=sm.robust.norms.TukeyBiweight())\n",
    "        results = model.fit()\n",
    "        out[f'{fieldName}linfitint'] = results.params[0]\n",
    "        out[f'{fieldName}alpha'] = results.params[1]\n",
    "        out[f'{fieldName}se1'] = results.bse[0]\n",
    "        out[f'{fieldName}se2'] = results.bse[1]\n",
    "        out[f'{fieldName}ssr'] = np.mean(results.resid ** 2)\n",
    "        out[f'{fieldName}resac1'] = CO_AutoCorr(results.resid, 1, 'Fourier')[0]\n",
    "    \n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

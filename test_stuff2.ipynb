{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1 = np.loadtxt(\"ts1.txt\")\n",
    "ts2 = np.loadtxt(\"ts2.txt\")\n",
    "ts3 = np.loadtxt(\"ts3.txt\")\n",
    "ts4 = np.loadtxt(\"ts4.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Operations.ST_LocalExtrema import ST_LocalExrema as STLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctypes import *\n",
    "import ctypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_file = \"/Users/joshua/Desktop/MS_shannon.so\"\n",
    "lib = CDLL(so_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.entropy.argtypes = [\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_int,\n",
    "    ctypes.c_int,\n",
    "    ctypes.c_int,\n",
    "    ctypes.POINTER(ctypes.c_double)\n",
    "]\n",
    "lib.entropy.restype = None  # void function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(data, bin_count, depth):\n",
    "    \"\"\"\n",
    "    Calculate Shannon Entropy using the C function.\n",
    "    \n",
    "    Args:\n",
    "    data (numpy.array): Input time series data\n",
    "    bin_count (int): Number of bins for encoding\n",
    "    depth (int): Depth of the encoding\n",
    "    \n",
    "    Returns:\n",
    "    float: Calculated Shannon Entropy\n",
    "    \"\"\"\n",
    "    data = np.ascontiguousarray(data, dtype=np.float64)\n",
    "    length = len(data)\n",
    "    result = ctypes.c_double()\n",
    "    \n",
    "    lib.entropy(data, length, bin_count, depth, ctypes.byref(result))\n",
    "    \n",
    "    return result.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.258990526199341"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shannon_entropy(ts3, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numBins = 10\n",
    "numBins = [numBins]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Operations.CO_Embed2_Shapes import CO_Embed2_Shapes as coes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Operations.CO_HistogramAMI import CO_HistogramAMI\n",
    "from Operations.CO_FirstCrossing import CO_FirstCrossing\n",
    "from Operations.IN_AutoMutualInfo import IN_AutoMutualInfo\n",
    "from Operations.CO_AutoCorr import CO_AutoCorr\n",
    "from PeripheryFunctions.BF_SignChange import BF_SignChange\n",
    "from PeripheryFunctions.BF_iszscored import BF_iszscored\n",
    "from scipy.optimize import curve_fit\n",
    "import warnings\n",
    "\n",
    "def CO_AddNoise(y, tau = 1, amiMethod = 'even', extraParam = None, randomSeed = None):\n",
    "    \"\"\"\n",
    "    CO_AddNoise: Changes in the automutual information with the addition of noise\n",
    "\n",
    "    Parameters:\n",
    "    y (array-like): The input time series (should be z-scored)\n",
    "    tau (int or str): The time delay for computing AMI (default: 1)\n",
    "    amiMethod (str): The method for computing AMI:\n",
    "                      'std1','std2','quantiles','even' for histogram-based estimation,\n",
    "                      'gaussian','kernel','kraskov1','kraskov2' for estimation using JIDT\n",
    "    extraParam: e.g., the number of bins input to CO_HistogramAMI, or parameter for IN_AutoMutualInfo\n",
    "    randomSeed (int): Settings for resetting the random seed for reproducible results\n",
    "\n",
    "    Returns:\n",
    "    dict: Statistics on the resulting set of automutual information estimates\n",
    "    \"\"\"\n",
    "\n",
    "    if not BF_iszscored(y):\n",
    "        warnings.warn(\"Input time series should be z-scored\")``\n",
    "    \n",
    "    # Set tau to minimum of autocorrelation function if 'ac' or 'tau'\n",
    "    if tau in ['ac', 'tau']:\n",
    "        tau = CO_FirstCrossing(y, 'ac', 0, 'discrete')\n",
    "    \n",
    "    # Generate noise\n",
    "    if randomSeed is not None:\n",
    "        np.random.seed(randomSeed)\n",
    "    noise = np.random.randn(len(y)) # generate uncorrelated additive noise\n",
    "\n",
    "    # Set up noise range\n",
    "    noiseRange = np.linspace(0, 3, 50) # compare properties across this noise range\n",
    "    numRepeats = len(noiseRange)\n",
    "\n",
    "    # Compute the automutual information across a range of noise levels\n",
    "    amis = np.zeros(numRepeats)\n",
    "    if amiMethod in ['std1', 'std2', 'quantiles', 'even']:\n",
    "        # histogram-based methods using my naive implementation in CO_Histogram\n",
    "        for i in range(numRepeats):\n",
    "            amis[i] = CO_HistogramAMI(y + noiseRange[i]*noise, tau, amiMethod, extraParam or 10) # use default numBins if None\n",
    "            if np.isnan(amis[i]):\n",
    "                raise ValueError('Error computing AMI: Time series too short (?)')\n",
    "    if amiMethod in ['gaussian','kernel','kraskov1','kraskov2']:\n",
    "        for i in range(numRepeats):\n",
    "            amis[i] = IN_AutoMutualInfo(y + noiseRange[i]*noise, tau, amiMethod, extraParam)\n",
    "            if np.isnan(amis[i]):\n",
    "                raise ValueError('Error computing AMI: Time series too short (?)')\n",
    "    \n",
    "    # Output statistics\n",
    "    out = {}\n",
    "    # Proportion decreases\n",
    "    out['pdec'] = np.sum(np.diff(amis) < 0) / (numRepeats - 1)\n",
    "\n",
    "    # Mean change in AMI\n",
    "    out['meanch'] = np.mean(np.diff(amis))\n",
    "\n",
    "    # Autocorrelation of AMIs\n",
    "    out['ac1'] = CO_AutoCorr(amis, 1, 'Fourier')[0]\n",
    "    out['ac2'] = CO_AutoCorr(amis, 2, 'Fourier')[0]\n",
    "\n",
    "    # Noise level required to reduce ami to proportion x of its initial value\n",
    "    firstUnderVals = [0.75, 0.50, 0.25]\n",
    "    for val in firstUnderVals:\n",
    "        out[f'firstUnder{val*100}'] = firstUnder_fn(val * amis[0], noiseRange, amis)\n",
    "\n",
    "    # AMI at actual noise levels: 0.5, 1, 1.5 and 2\n",
    "    noiseLevels = [0.5, 1, 1.5, 2]\n",
    "    for nlvl in noiseLevels:\n",
    "        out[f'ami_at_{nlvl*10}'] = amis[np.argmax(noiseRange >= nlvl)]\n",
    "\n",
    "    # Count number of times the AMI function crosses its mean\n",
    "    out['pcrossmean'] = np.sum(np.diff(np.sign(amis - np.mean(amis))) != 0) / (numRepeats - 1)\n",
    "\n",
    "    # Fit exponential decay\n",
    "    expFunc = lambda x, a, b : a * np.exp(b * x)\n",
    "    popt, pcov = curve_fit(expFunc, noiseRange, amis, p0=[amis[0], -1])\n",
    "    out['fitexpa'], out['fitexpb'] = popt\n",
    "    residuals = amis - expFunc(noiseRange, *popt)\n",
    "    ss_res = np.sum(residuals**2)\n",
    "    ss_tot = np.sum((amis - np.mean(amis))**2)\n",
    "    out['fitexpr2'] = 1 - (ss_res / ss_tot)\n",
    "    out['fitexpadjr2'] = 1 - (1-out['fitexpr2'])*(len(amis)-1)/(len(amis)-2-1)\n",
    "    out['fitexprmse'] = np.sqrt(np.mean(residuals**2))\n",
    "\n",
    "    # Fit linear function\n",
    "    p = np.polyfit(noiseRange, amis, 1)\n",
    "    out['fitlina'], out['fitlinb'] = p\n",
    "    lin_fit = np.polyval(p, noiseRange)\n",
    "    out['linfit_mse'] = np.mean((lin_fit - amis)**2)\n",
    "\n",
    "    return out\n",
    "\n",
    "# helper functions\n",
    "def firstUnder_fn(x, m, p):\n",
    "    \"\"\"\n",
    "    Find the value of m for the first time p goes under the threshold, x. \n",
    "    p and m vectors of the same length\n",
    "    \"\"\"\n",
    "    first_i = next((m_val for m_val, p_val in zip(m, p) if p_val < x), m[-1])\n",
    "    return first_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_intervals(elements, scales):\n",
    "    return np.array([int((elements / (1 << scale)) + 0.5) for scale in range(scales - 1, -1, -1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfa(x, intervals):\n",
    "    elements = len(x)\n",
    "    flucts = np.zeros(len(intervals))\n",
    "\n",
    "    for scale, interval in enumerate(intervals):\n",
    "        subdivs = int(np.ceil(elements / interval))\n",
    "        trend = np.zeros(elements)\n",
    "\n",
    "        for i in range(subdivs):\n",
    "            start = i * interval\n",
    "            end = start + interval\n",
    "            if end > elements:\n",
    "                trend[start:] = x[start:]\n",
    "                break\n",
    "            segment = x[start:end]\n",
    "            t = np.arange(interval)\n",
    "            coeffs = np.polyfit(t, segment, 1)\n",
    "            trend[start:end] = np.polyval(coeffs, t)\n",
    "        flucts[scale] = np.sqrt(np.mean((x - trend)**2))\n",
    "\n",
    "    return flucts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastdfa(x, intervals=None):\n",
    "    \"\"\"\n",
    "    Perform fast detrended fluctuation analysis on a nonstationary input signal.\n",
    "\n",
    "    Args:\n",
    "    x: Input signal (must be a 1D numpy array)\n",
    "    intervals: Optional list of sample interval widths at each scale\n",
    "\n",
    "    Returns:\n",
    "    intervals: List of sample interval widths at each scale\n",
    "    flucts: List of fluctuation amplitudes at each scale\n",
    "    \"\"\"\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(\"Input sequence must be a vector.\")\n",
    "    \n",
    "    elements = len(x)\n",
    "    \n",
    "    if intervals is None:\n",
    "        scales = int(np.log2(elements))\n",
    "        if (1 << (scales - 1)) > elements / 2.5:\n",
    "            scales -= 1\n",
    "        intervals = _calculate_intervals(elements, scales)\n",
    "    else:\n",
    "        if len(intervals) < 2:\n",
    "            raise ValueError(\"Number of intervals must be greater than one.\")\n",
    "        if np.any((intervals > elements) | (intervals < 3)):\n",
    "            raise ValueError(\"Invalid interval size: must be between size of sequence x and 3.\")\n",
    "    \n",
    "    y = np.cumsum(x)\n",
    "    flucts = dfa(y, intervals)\n",
    "    \n",
    "    return intervals, flucts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpts, ypts = fastdfa(ts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = np.polyfit(np.log10(xpts), np.log10(ypts), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.61481407, -0.93273621])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Operations.SC_fastdfa import SC_fastdfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.614814074638312"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SC_fastdfa(ts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _buffer(X, n, p=0, opt=None):\n",
    "    '''Mimic MATLAB routine to generate buffer array\n",
    "\n",
    "    MATLAB docs here: https://se.mathworks.com/help/signal/ref/buffer.html.\n",
    "    Taken from: https://stackoverflow.com/questions/38453249/does-numpy-have-a-function-equivalent-to-matlabs-buffer \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: ndarray\n",
    "        Signal array\n",
    "    n: int\n",
    "        Number of data segments\n",
    "    p: int\n",
    "        Number of values to overlap\n",
    "    opt: str\n",
    "        Initial condition options. default sets the first `p` values to zero,\n",
    "        while 'nodelay' begins filling the buffer immediately.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : (n,n) ndarray\n",
    "        Buffer array created from X\n",
    "    '''\n",
    "    import numpy as np\n",
    "\n",
    "    if opt not in [None, 'nodelay']:\n",
    "        raise ValueError('{} not implemented'.format(opt))\n",
    "\n",
    "    i = 0\n",
    "    first_iter = True\n",
    "    while i < len(X):\n",
    "        if first_iter:\n",
    "            if opt == 'nodelay':\n",
    "                # No zeros at array start\n",
    "                result = X[:n]\n",
    "                i = n\n",
    "            else:\n",
    "                # Start with `p` zeros\n",
    "                result = np.hstack([np.zeros(p), X[:n-p]])\n",
    "                i = n-p\n",
    "            # Make 2D array and pivot\n",
    "            result = np.expand_dims(result, axis=0).T\n",
    "            first_iter = False\n",
    "            continue\n",
    "\n",
    "        # Create next column, add `p` results from last col if given\n",
    "        col = X[i:i+(n-p)]\n",
    "        if p != 0:\n",
    "            col = np.hstack([result[:,-1][-p:], col])\n",
    "        i += n-p\n",
    "\n",
    "        # Append zeros if last row and not length `n`\n",
    "        if len(col) < n:\n",
    "            col = np.hstack([col, np.zeros(n-len(col))])\n",
    "\n",
    "        # Combine result with next row\n",
    "        result = np.hstack([result, np.expand_dims(col, axis=0).T])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Operations.CO_AutoCorr import CO_AutoCorr\n",
    "from warnings import warn\n",
    "from scipy.interpolate import CubicSpline\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def SC_FluctAnal(x, q = 2, wtf = 'rsrange', tauStep = 1, k = 1, lag = None, logInc = True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(x) # time series length\n",
    "\n",
    "    # Compute integrated sequence\n",
    "    if lag is None or lag == 1:\n",
    "        y = np.cumsum(x) # normal cumulative sum\n",
    "    else:\n",
    "        y = np.cumsum(x[::lag]) # if a lag is specified, do a decimation\n",
    "    \n",
    "    # Perform scaling over a range of tau, up to a fifth the time-series length\n",
    "    #-------------------------------------------------------------------------------\n",
    "    # Peng (1995) suggests 5:N/4 for DFA\n",
    "    # Caccia suggested from 10 to (N-1)/2...\n",
    "    #-------------------------------------------------------------------------------\n",
    "\n",
    "    if logInc:\n",
    "        # in this case tauStep is the number of points to compute\n",
    "        if tauStep == 1:\n",
    "            # handle the case where tauStep is 1, but we want to take the upper \n",
    "            # limit (MATLAB) rather than the lower (Python)\n",
    "            tauStep += 1\n",
    "            logRange = np.linspace(np.log(5), np.log(np.floor(N/2)), tauStep)[1] # take the second entry (upper limit)\n",
    "        else:\n",
    "            logRange = np.linspace(np.log(5), np.log(np.floor(N/2)), tauStep)\n",
    "        taur = np.unique(np.round(np.exp(logRange)).astype(int))\n",
    "    else:\n",
    "        taur = np.arange(5, int(np.floor(N/2)) + 1, tauStep)\n",
    "    ntau = len(taur) # % analyze the time series across this many timescales\n",
    "    #print(taur)\n",
    "    if ntau < 8: # fewer than 8 points\n",
    "        # time series is too short for analysing using this fluctuation analysis. \n",
    "        warn(f\"This time series (N = {N}) is too short to analyze using this fluctuation analysis.\")\n",
    "        out = np.NaN\n",
    "    \n",
    "    # 2) Compute the fluctuation function, F\n",
    "    F = np.zeros(ntau)\n",
    "    # each entry corresponds to a given scale, tau\n",
    "    for i in range(ntau):\n",
    "        # buffer the time series at the scale tau\n",
    "        tau = taur[i]\n",
    "        y_buff = _buffer(y, tau)\n",
    "        if y_buff.shape[1] > int(np.floor(N/tau)): # zero-padded, remove trailing set of pts...\n",
    "            y_buff = y_buff[:, :-1]\n",
    "\n",
    "        # analyzed length of time series (with trailing pts removed)\n",
    "        nn = y_buff.shape[1] * tau\n",
    "\n",
    "        if wtf == 'nothing':\n",
    "            y_dt = y_buff.reshape(nn, 1)\n",
    "        elif wtf == 'endptdiff':\n",
    "            # look at differences in end-points in each subsegment\n",
    "            y_dt = y_buff[-1, :] - y_buff[0, :]\n",
    "        elif wtf == 'range':\n",
    "            y_dt = np.ptp(y_buff, axis=0)\n",
    "        elif wtf == 'std':\n",
    "            y_dt = np.std(y_buff, ddof=1, axis=0)\n",
    "        elif wtf == 'iqr':\n",
    "            y_dt = np.percentile(y_buff, 75, method='hazen', axis=0) - np.percentile(y_buff, 25, method='hazen', axis=0)\n",
    "        elif wtf == 'dfa':\n",
    "            tt = np.arange(1, tau + 1)[:, np.newaxis]\n",
    "            for j in range(y_buff.shape[1]):\n",
    "                p = np.polyfit(tt.ravel(), y_buff[:, j], k)\n",
    "                y_buff[:, j] -= np.polyval(p, tt).ravel()\n",
    "            y_dt = y_buff.reshape(-1)\n",
    "        elif wtf == 'rsrange':\n",
    "            # Remove straight line first: Caccia et al. Physica A, 1997\n",
    "            # Straight line connects end points of each window:\n",
    "            b = y_buff[0, :]\n",
    "            m = y_buff[-1, :] - b\n",
    "            y_buff -= (np.linspace(0, 1, tau)[:, np.newaxis] * m + b)\n",
    "            y_dt = np.ptp(y_buff, axis=0)\n",
    "        elif wtf == 'rsrangefit':\n",
    "            # polynomial fit (order k) rather than endpoints fit: (~DFA)\n",
    "            tt = np.arange(1, tau + 1)[:, np.newaxis]\n",
    "            for j in range(y_buff.shape[1]):\n",
    "                p = np.polyfit(tt.ravel(), y_buff[:, j], k)\n",
    "                y_buff[:, j] -= np.polyval(p, tt).ravel()\n",
    "            y_dt = np.ptp(y_buff, axis=0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fluctuation analysis method '{wtf}\")\n",
    "        \n",
    "        F[i] = (np.mean(y_dt**q))**(1/q)\n",
    "\n",
    "    # Smooth unevenly-distributed points in log space\n",
    "    if logInc:\n",
    "        logtt = np.log(taur)\n",
    "        logFF = np.log(F)\n",
    "        numTimeScales = ntau\n",
    "    else:\n",
    "        # need to smooth the unevenly-distributed pts (using a spline)\n",
    "        logtaur = np.log(taur)\n",
    "        logF = np.log(F)\n",
    "        numTimeScales = 50 # number of sampling pts across the range\n",
    "        logtt = np.linspace(np.min(logtaur), np.max(logtaur), numTimeScales) # even sampling in tau\n",
    "        # equivalent to spline function in MATLAB\n",
    "        spl_fit = CubicSpline(logtaur, logF)\n",
    "        logFF = spl_fit(logtt)\n",
    "\n",
    "    # Linear fit the log-log plot: full range\n",
    "    out = {}\n",
    "    out = doRobustLinearFit(out, logtt, logFF, range(numTimeScales), '')\n",
    "    \n",
    "    \"\"\" \n",
    "    WE NEED SOME SORT OF AUTOMATIC DETECTION OF GRADIENT CHANGES/NUMBER\n",
    "    %% OF PIECEWISE LINEAR PIECES\n",
    "\n",
    "    ------------------------------------------------------------------------------\n",
    "    Try assuming two components (2 distinct scaling regimes)\n",
    "    ------------------------------------------------------------------------------\n",
    "    Move through, and fit a straight line to loglog before and after each point.\n",
    "    Find point with the minimum sum of squared errors\n",
    "\n",
    "    First spline interpolate to get an even sampling of the interval\n",
    "    (currently, in the log scale, there are relatively more at slower timescales)\n",
    "\n",
    "    Determine the errors\n",
    "    \"\"\"\n",
    "    sserr = np.full(numTimeScales, np.nan) # don't choose the end pts\n",
    "    minPoints = 6\n",
    "    for i in range(minPoints, (numTimeScales-minPoints)+1):\n",
    "        r1 = np.arange(i)\n",
    "        p1 = np.polyfit(logtt[r1], logFF[r1], 1) # first degree polynomial\n",
    "        r2 = np.arange(i-1, numTimeScales)\n",
    "        p2 = np.polyfit(logtt[r2], logFF[r2], 1)\n",
    "        sserr[i] = (np.linalg.norm(np.polyval(p1, logtt[r1]) - logFF[r1]) +\n",
    "                    np.linalg.norm(np.polyval(p2, logtt[r2]) - logFF[r2]))\n",
    "    \n",
    "    # breakPt is the point where it's best to fit a line before and another line after\n",
    "    breakPt = np.nanargmin(sserr)\n",
    "    r1 = np.arange(breakPt)\n",
    "    r2 = np.arange(breakPt-1, numTimeScales)\n",
    "\n",
    "    # Proportion of the domain of timescales corresponding to the first good linear fit\n",
    "    out['prop_r1'] = len(r1)/numTimeScales\n",
    "    out['logtausplit'] = logtt[breakPt-1]\n",
    "    out['ratsplitminerr'] = np.nanmin(sserr) / out['ssr']\n",
    "    out['meanssr'] = np.nanmean(sserr)\n",
    "    out['stdssr'] = np.nanstd(sserr, ddof=1)\n",
    "\n",
    "    # Check that at least 3 points are available\n",
    "    # Now we perform the robust linear fitting and get statistics on the two segments\n",
    "    # R1\n",
    "    out = doRobustLinearFit(out, logtt, logFF, r1, 'r1_')\n",
    "    # R2\n",
    "    out = doRobustLinearFit(out, logtt, logFF, r2, 'r2_')\n",
    "\n",
    "    if np.isnan(out['r1_alpha']) or np.isnan(out['r2_alpha']):\n",
    "        out['alpharat'] = np.nan\n",
    "    else:\n",
    "        out['alpharat'] = out['r1_alpha'] / out['r2_alpha']\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doRobustLinearFit(out, logtt, logFF, theRange, fieldName):\n",
    "    \"\"\"\n",
    "    Get robust linear fit statistics on scaling range\n",
    "    Adds fields to the output structure\n",
    "    \"\"\"\n",
    "    if len(theRange) < 8 or np.all(np.isnan(logFF[theRange])):\n",
    "        out[f'{fieldName}linfitint'] = np.nan\n",
    "        out[f'{fieldName}alpha'] = np.nan\n",
    "        out[f'{fieldName}se1'] = np.nan\n",
    "        out[f'{fieldName}se2'] = np.nan\n",
    "        out[f'{fieldName}ssr'] = np.nan\n",
    "        out[f'{fieldName}resac1'] = np.nan\n",
    "    else:\n",
    "        X = sm.add_constant(logtt[theRange])\n",
    "        model = sm.RLM(logFF[theRange], X, M=sm.robust.norms.TukeyBiweight())\n",
    "        results = model.fit()\n",
    "        out[f'{fieldName}linfitint'] = results.params[0]\n",
    "        out[f'{fieldName}alpha'] = results.params[1]\n",
    "        out[f'{fieldName}se1'] = results.bse[0]\n",
    "        out[f'{fieldName}se2'] = results.bse[1]\n",
    "        out[f'{fieldName}ssr'] = np.mean(results.resid ** 2)\n",
    "        out[f'{fieldName}resac1'] = CO_AutoCorr(results.resid, 1, 'Fourier')[0]\n",
    "    \n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from warnings import warn \n",
    "\n",
    "def SSC_MMA(y, doOverlap = False, scaleRange = None, qRange = None):\n",
    "    \"\"\"\n",
    "    Python implementation of multiscale multifractal analysis (MMA)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y : array_like\n",
    "        Input time series\n",
    "    do_overlap : bool, optional\n",
    "        Whether to use overlapping windows (default is False)\n",
    "    scale_range : tuple, optional\n",
    "        (min_scale, max_scale) for analysis (default is None)\n",
    "    q_range : tuple, optional\n",
    "        (q_min, q_max) for analysis (default is None)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing various MMA statistics\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(y)\n",
    "\n",
    "    if scaleRange is None:\n",
    "        scaleRange = [10, np.ceil(N/40)]\n",
    "\n",
    "    minScale = scaleRange[0]\n",
    "    maxScale = scaleRange[1]\n",
    "    if (maxScale/5) < minScale:\n",
    "        warn(f\"Time-series (N={N}) too short for multiscale multifractal analysis\")\n",
    "        out = np.NaN\n",
    "    elif maxScale % 5 != 0:\n",
    "        maxScale = np.ceil(maxScale/5)*5\n",
    "    \n",
    "    if qRange is None:\n",
    "        qRange = [-5, 5]\n",
    "    qMin, qMax = qRange\n",
    "    stepVal = 0.1\n",
    "\n",
    "    qList = np.arange(start=qMin, stop=qMax+stepVal, step=stepVal)\n",
    "    qList[qList == 0] = 0.0001\n",
    "\n",
    "    prof = np.cumsum(y)\n",
    "    numIncrements = 20\n",
    "\n",
    "    sListFull = np.unique(np.round(np.linspace(minScale, maxScale, numIncrements)))\n",
    "    fqs = []\n",
    "    for s in sListFull:\n",
    "        if doOverlap:\n",
    "            coordinates = np.array([np.arange(i, i+s) for i in range(len(prof)-s+1)])\n",
    "        else:            \n",
    "            coordinates = [prof[i:i+int(s)] for i in range(0, len(prof)-int(s)+1, int(s))]\n",
    "        print(coordinates)\n",
    "        segments = prof[coordinates]\n",
    "        x_base = np.arange(1, s+1)\n",
    "        f2nis = []\n",
    "\n",
    "        for seg in segments:\n",
    "            fit = np.polyfit(x_base, seg, 2)\n",
    "            variance = np.mean((seg - np.polyval(fit, x_base))**2)\n",
    "            f2nis.append(variance)\n",
    "        \n",
    "        f2nis = np.array(f2nis)\n",
    "\n",
    "        for q in qList:\n",
    "            fqs.append([q, s, (np.mean(f2nis**(q/2)))**(1/q)])\n",
    "        \n",
    "    fqs = np.array(fqs)\n",
    "    fqsll = np.column_stack((fqs[:, 0], fqs[:, 1], np.log(fqs[:, 1]), np.log(fqs[:, 2])))\n",
    "    \n",
    "    print(fqs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def CO_TranslateShape(y, shape = 'circle', d = 2, howToMove = 'pts'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "\n",
    "    # add a time index\n",
    "    ty = np.column_stack((np.arange(1, N+1), y))\n",
    "    #-------------------------------------------------------------------------------\n",
    "    # Generate the statistics on the number of points inside the shape as it is\n",
    "    # translated across the time series\n",
    "    #-------------------------------------------------------------------------------\n",
    "    if howToMove == 'pts':\n",
    "        if shape == 'circle':\n",
    "            r = d\n",
    "            w = int(np.floor(r))\n",
    "            rnge = np.arange(1 + w, N - w)\n",
    "            NN = len(rnge)\n",
    "            np_array = np.zeros(NN)\n",
    "\n",
    "            for i in range(NN):\n",
    "                win = ty[rnge[i] - w:rnge[i] + w + 1, :]\n",
    "                difwin = win - np.ones((2*w+1, 1)) * ty[rnge[i], :]\n",
    "                np_array[i] = np.sum(np.sum(difwin**2, axis=1) <= r**2)\n",
    "           \n",
    "        elif shape == 'rectangle':\n",
    "            w = d\n",
    "            rnge = np.arange(1 + d, N - d)\n",
    "            NN = len(rnge)\n",
    "            np_array = np.zeros(NN)\n",
    "            for i in range(NN):\n",
    "                np_array[i] = np.sum(np.abs(y[rnge[i] - d:rnge[i] + d + 1]) <= np.abs(y[rnge[i]]))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown shape '{shape}'\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown setting for howToMove: '{howToMove}'\")\n",
    "    \n",
    "    # statistics on the number of hits inside in the shape\n",
    "    out = {}\n",
    "    out['max'] = np.max(np_array)\n",
    "    out['std'] = np.std(np_array, ddof=1)\n",
    "    out['mean'] = np.mean(np_array)\n",
    "\n",
    "    # Count the hits\n",
    "    unique, counts = np.unique(np_array, return_counts=True)\n",
    "    histnp = dict(zip(unique, counts))\n",
    "\n",
    "    # Compute the mode of the histogram\n",
    "    mode_val = max(histnp, key=histnp.get)\n",
    "    out['npatmode'] = histnp[mode_val] / NN\n",
    "    out['mode'] = mode_val\n",
    "\n",
    "    for i in range(1, min(12, 2*w+2)):\n",
    "        out[f\"{['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'eleven'][i-1]}s\"] \\\n",
    "            = np.mean(np_array == i)\n",
    "    \n",
    "    # Stationarity of the statistics in 2,3, & 4 segments of the time series:\n",
    "    # NOT YET IMPLEMENTED - DEPENDS ON SY_SLIDINGWINDOW\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max': 3.0,\n",
       " 'std': 0.0,\n",
       " 'mean': 3.0,\n",
       " 'npatmode': 1.0,\n",
       " 'mode': 3.0,\n",
       " 'ones': 0.0,\n",
       " 'twos': 0.0,\n",
       " 'threes': 1.0,\n",
       " 'fours': 0.0,\n",
       " 'fives': 0.0}"
      ]
     },
     "execution_count": 936,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CO_TranslateShape(ts1, shape='circle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47077312, 0.49536432, 0.51661102, 0.53418911, 0.54789987,\n",
       "       0.55767468, 0.56357235, 0.56576941, 0.5645444 , 0.56025776,\n",
       "       0.55332889, 0.54421224, 0.53337411, 0.52127155, 0.5083346 ,\n",
       "       0.4949524 , 0.48146371, 0.46815152, 0.4552416 , 0.4429043 ,\n",
       "       0.4312588 , 0.42037921, 0.4103016 , 0.40103139, 0.3925506 ,\n",
       "       0.38482445, 0.37780715, 0.3714467 , 0.36568874, 0.36047937,\n",
       "       0.35576716, 0.35150451, 0.34764829, 0.34416021, 0.34100676,\n",
       "       0.33815899, 0.33559221, 0.33328549, 0.33122131, 0.32938508,\n",
       "       0.32776477, 0.3263505 , 0.32513424, 0.32410943, 0.3232708 ,\n",
       "       0.32261401, 0.32213551, 0.32183233, 0.32170191, 0.32174202,\n",
       "       0.32195068, 0.32232616, 0.32286702, 0.32357218, 0.32444108,\n",
       "       0.32547386, 0.32667158, 0.32803644, 0.3295721 , 0.33128393,\n",
       "       0.33317933, 0.33526801, 0.33756231, 0.34007753, 0.34283227,\n",
       "       0.34584873, 0.34915306, 0.35277569, 0.3567515 , 0.36112002,\n",
       "       0.36592527, 0.37121548, 0.37704223, 0.38345913, 0.39051987,\n",
       "       0.3982753 , 0.40676972, 0.41603612, 0.42609055, 0.43692558,\n",
       "       0.44850323, 0.46074775, 0.47353869, 0.48670499, 0.50002082,\n",
       "       0.51320385, 0.52591682, 0.53777284, 0.54834489, 0.5571796 ,\n",
       "       0.56381486, 0.5678008 , 0.56872276, 0.56622504, 0.56003359,\n",
       "       0.54997603, 0.53599716, 0.51816852, 0.49669109, 0.47189046])"
      ]
     },
     "execution_count": 951,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kde_fit = stats.gaussian_kde(ts1, bw_method='scott')\n",
    "xi = np.linspace(np.min(ts1), np.max(ts1), 100)\n",
    "f = kde_fit(xi)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FitKernelSmooth(x):\n",
    "    \n",
    "    m = np.mean(x)\n",
    "    kdefit = stats.gaussian_kde(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [],
   "source": [
    "import antropy as ant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4299028461250582"
      ]
     },
     "execution_count": 960,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ant.perm_entropy(ts2, order=3, delay=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [],
   "source": [
    "import antropy as ant\n",
    "from antropy.utils import _embed, _xlogx\n",
    "from math import factorial\n",
    "import numpy as np\n",
    "from Operations.CO_FirstCrossing import CO_FirstCrossing\n",
    "\n",
    "def _perm_entropy_all(x, order=3, delay=1, normalize=False, return_normedCounts=False):\n",
    "    # compute all relevant perm entropy stats\n",
    "    if isinstance(delay, (list, np.ndarray, range)):\n",
    "        return np.mean([_perm_entropy_all(x, order=order, delay=d, normalize=normalize) for d in delay])\n",
    "    x = np.array(x)\n",
    "    ran_order = range(order)\n",
    "    hashmult = np.power(order, ran_order)\n",
    "    assert delay > 0, \"delay must be greater than zero.\"\n",
    "    # Embed x and sort the order of permutations\n",
    "    sorted_idx = _embed(x, order=order, delay=delay).argsort(kind=\"quicksort\")\n",
    "    # Associate unique integer to each permutations\n",
    "    hashval = (np.multiply(sorted_idx, hashmult)).sum(1)\n",
    "    # Return the counts\n",
    "    _, c = np.unique(hashval, return_counts=True)\n",
    "    # Use np.true_divide for Python 2 compatibility\n",
    "    p = np.true_divide(c, c.sum())\n",
    "    pe = -_xlogx(p).sum()\n",
    "    if normalize:\n",
    "        pe /= np.log2(factorial(order))\n",
    "    \n",
    "    if return_normedCounts:\n",
    "        return pe, p\n",
    "    else:\n",
    "        return pe\n",
    "\n",
    "def EN_PermEn(y, m = 2, tau = 1):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if tau == 'ac':\n",
    "        tau = CO_FirstCrossing(y, 'ac', 0, 'discrete')\n",
    "    elif not isinstance(tau, int):\n",
    "        raise TypeError(\"Invalid type for tau. Can be either 'ac' or an integer.\")\n",
    "    \n",
    "    pe, p = _perm_entropy_all(y, order=m, delay=tau, normalize=False, return_normedCounts=True)\n",
    "    pe_n = ant.perm_entropy(y, order=m, delay=tau, normalize=True)\n",
    "    Nx = len(y) - (m-1) * tau # get the number of embedding vectors\n",
    "    # p will only contain non-zero probabilities, so to make the output consistent with MATLAB, we need to add a correction:\n",
    "    # not saying this is correct, but this is how it is implemented in MATLAB and this is a port...\n",
    "    lenP = len(p)\n",
    "    numZeros = factorial(m) - lenP\n",
    "    # append the zeros to the end of p\n",
    "    p = np.concatenate([np.array(p), np.zeros(numZeros)])\n",
    "    p_LE = [np.maximum(1/Nx, p[i]) for i in range(len(p))]\n",
    "    permEnLE = -np.sum(p_LE * np.log(p_LE))/(m-1)\n",
    "\n",
    "    out = {}\n",
    "    out['permEn'] = pe\n",
    "    out['normPermEn'] = pe_n\n",
    "    out['permEnLE'] = permEnLE\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'permEn': 3.0946435887719645,\n",
       " 'normPermEn': 0.6749550488766738,\n",
       " 'permEnLE': 0.7479272237443636}"
      ]
     },
     "execution_count": 1018,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EN_PermEn(ts1, 4, 'ac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Operations.EN_PermEn import EN_PermEn as EPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1016], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mEPE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mye\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/py-hctsa-project/Operations/EN_PermEn.py:34\u001b[0m, in \u001b[0;36mEN_PermEn\u001b[0;34m(y, m, tau)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mEN_PermEn\u001b[39m(y, m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, tau \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     pe, p \u001b[38;5;241m=\u001b[39m \u001b[43m_perm_entropy_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_normedCounts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     pe_n \u001b[38;5;241m=\u001b[39m ant\u001b[38;5;241m.\u001b[39mperm_entropy(y, order\u001b[38;5;241m=\u001b[39mm, delay\u001b[38;5;241m=\u001b[39mtau, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m     Nx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y) \u001b[38;5;241m-\u001b[39m (m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m tau \u001b[38;5;66;03m# get the number of embedding vectors\u001b[39;00m\n",
      "File \u001b[0;32m~/py-hctsa-project/Operations/EN_PermEn.py:13\u001b[0m, in \u001b[0;36m_perm_entropy_all\u001b[0;34m(x, order, delay, normalize, return_normedCounts)\u001b[0m\n\u001b[1;32m     11\u001b[0m ran_order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(order)\n\u001b[1;32m     12\u001b[0m hashmult \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpower(order, ran_order)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mdelay\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelay must be greater than zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Embed x and sort the order of permutations\u001b[39;00m\n\u001b[1;32m     15\u001b[0m sorted_idx \u001b[38;5;241m=\u001b[39m _embed(x, order\u001b[38;5;241m=\u001b[39morder, delay\u001b[38;5;241m=\u001b[39mdelay)\u001b[38;5;241m.\u001b[39margsort(kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquicksort\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "EPE(ts1, 5, 'ye')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 1007,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factorial(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

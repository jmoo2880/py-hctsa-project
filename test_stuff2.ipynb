{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1 = np.loadtxt(\"ts1.txt\")\n",
    "ts2 = np.loadtxt(\"ts2.txt\")\n",
    "ts3 = np.loadtxt(\"ts3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Operations.ST_LocalExtrema import ST_LocalExrema as STLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctypes import *\n",
    "import ctypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_file = \"/Users/joshua/Desktop/MS_shannon.so\"\n",
    "lib = CDLL(so_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.entropy.argtypes = [\n",
    "    np.ctypeslib.ndpointer(dtype=np.float64, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_int,\n",
    "    ctypes.c_int,\n",
    "    ctypes.c_int,\n",
    "    ctypes.POINTER(ctypes.c_double)\n",
    "]\n",
    "lib.entropy.restype = None  # void function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(data, bin_count, depth):\n",
    "    \"\"\"\n",
    "    Calculate Shannon Entropy using the C function.\n",
    "    \n",
    "    Args:\n",
    "    data (numpy.array): Input time series data\n",
    "    bin_count (int): Number of bins for encoding\n",
    "    depth (int): Depth of the encoding\n",
    "    \n",
    "    Returns:\n",
    "    float: Calculated Shannon Entropy\n",
    "    \"\"\"\n",
    "    data = np.ascontiguousarray(data, dtype=np.float64)\n",
    "    length = len(data)\n",
    "    result = ctypes.c_double()\n",
    "    \n",
    "    lib.entropy(data, length, bin_count, depth, ctypes.byref(result))\n",
    "    \n",
    "    return result.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.258990526199341"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shannon_entropy(ts3, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numBins = 10\n",
    "numBins = [numBins]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from Operations.CO_FirstCrossing import CO_FirstCrossing\n",
    "from Operations.CO_AutoCorr import CO_AutoCorr\n",
    "from numpy import histogram_bin_edges\n",
    "\n",
    "def CO_Embed2_Shapes(y, tau = 'tau', shape = 'circle', r = 1):\n",
    "    \"\"\"\n",
    "    Shape-based statistics in a 2-d embedding space.\n",
    "\n",
    "    Takes a shape and places it on each point in the two-dimensional time-delay\n",
    "    embedding space sequentially. This function counts the points inside this shape\n",
    "    as a function of time, and returns statistics on this extracted time series.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y : array_like\n",
    "        The input time-series as a (z-scored) column vector.\n",
    "    tau : int or str, optional\n",
    "        The time-delay. If 'tau', it's set to the first zero crossing of the autocorrelation function.\n",
    "    shape : str, optional\n",
    "        The shape to use. Currently only 'circle' is supported.\n",
    "    r : float, optional\n",
    "        The radius of the circle.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary containing various statistics of the constructed time series.\n",
    "    \"\"\"\n",
    "    if tau == 'tau':\n",
    "        tau = CO_FirstCrossing(y, 'ac', 0, 'discrete')\n",
    "        # cannot set time delay > 10% of the length of the time series...\n",
    "        if tau > len(y)/10:\n",
    "            tau = int(np.floor(len(y)/10))\n",
    "        \n",
    "    # Create the recurrence space, populated by points m\n",
    "    m = np.column_stack((y[:-tau], y[tau:]))\n",
    "    N = len(m)\n",
    "\n",
    "    # Start the analysis\n",
    "    counts = np.zeros(N)\n",
    "    if shape == 'circle':\n",
    "        # Puts a circle around each point in the embedding space in turn\n",
    "        # counts how many pts are inside this shape, looks at the time series thus formed\n",
    "        for i in range(N): # across all pts in the time series\n",
    "            m_c = m - m[i] # pts wrt current pt i\n",
    "            m_c_d = np.sum(m_c**2, axis=1) # Euclidean distances from pt i\n",
    "            counts[i] = np.sum(m_c_d <= r**2) # number of pts enclosed in a circle of radius r\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown shape '{shape}'\")\n",
    "    \n",
    "    counts -= 1 # ignore self counts\n",
    "\n",
    "    if np.all(counts == 0):\n",
    "        print(\"No counts detected!\")\n",
    "        return np.nan\n",
    "\n",
    "    # Return basic statistics on the counts\n",
    "    out = {}\n",
    "    out['ac1'] = CO_AutoCorr(counts, 1, 'Fourier')[0]\n",
    "    out['ac2'] = CO_AutoCorr(counts, 2, 'Fourier')[0]\n",
    "    out['ac3'] = CO_AutoCorr(counts, 3, 'Fourier')[0]\n",
    "    out['tau'] = CO_FirstCrossing(counts, 'ac', 0, 'continuous')\n",
    "    out['max'] = np.max(counts)\n",
    "    out['std'] = np.std(counts, ddof=1)\n",
    "    out['median'] = np.median(counts)\n",
    "    out['mean'] = np.mean(counts)\n",
    "    out['iqr'] = np.percentile(counts, 75, method='hazen') - np.percentile(counts, 25, method='hazen')\n",
    "    out['iqronrange'] = out['iqr']/np.ptp(counts)\n",
    "\n",
    "    # distribution - using sqrt binning method\n",
    "    edges = histogram_bin_edges(counts, bins='sqrt')\n",
    "    binCounts, binEdges = np.histogram(counts, bins=edges)\n",
    "    # normalise bin counts\n",
    "    binCountsNorm = np.divide(binCounts, np.sum(binCounts))\n",
    "    print(len(binCountsNorm))\n",
    "    # get bin centres\n",
    "    binCentres = (binEdges[:-1] + binEdges[1:]) / 2\n",
    "    out['mode_val'] = np.max(binCountsNorm)\n",
    "    out['mode'] = binCentres[np.argmax(binCountsNorm)]\n",
    "    # histogram entropy\n",
    "    out['hist_ent'] = np.sum(binCountsNorm[binCountsNorm > 0] * np.log(binCountsNorm[binCountsNorm > 0]))\n",
    "\n",
    "    # Stationarity measure for fifths of the time series\n",
    "    afifth = int(np.floor(N/5))\n",
    "    buffer_m = np.array([counts[i*afifth:(i+1)*afifth] for i in range(5)])\n",
    "    out['statav5_m'] = np.std(np.mean(buffer_m, axis=1), ddof=1) / np.std(counts, ddof=1)\n",
    "    out['statav5_s'] = np.std(np.std(buffer_m, axis=1), ddof=1) / np.std(counts, ddof=1)\n",
    "\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ac1': 0.23392250602472692,\n",
       " 'ac2': 0.005446701506938792,\n",
       " 'ac3': -0.013460515225042154,\n",
       " 'tau': 2.288075266928414,\n",
       " 'max': 10.0,\n",
       " 'std': 2.2532646204888898,\n",
       " 'median': 2.0,\n",
       " 'mean': 2.5265265265265264,\n",
       " 'iqr': 3.0,\n",
       " 'iqronrange': 0.3,\n",
       " 'mode_val': 0.2122122122122122,\n",
       " 'mode': 0.15625,\n",
       " 'hist_ent': -2.0469340846150486,\n",
       " 'statav5_m': 0.05265712720156759,\n",
       " 'statav5_s': 0.040588662251160465}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CO_Embed2_Shapes(ts3, r=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Operations.CO_HistogramAMI import CO_HistogramAMI\n",
    "from Operations.CO_FirstCrossing import CO_FirstCrossing\n",
    "from Operations.IN_AutoMutualInfo import IN_AutoMutualInfo\n",
    "from Operations.CO_AutoCorr import CO_AutoCorr\n",
    "from PeripheryFunctions.BF_SignChange import BF_SignChange\n",
    "from PeripheryFunctions.BF_iszscored import BF_iszscored\n",
    "from scipy.optimize import curve_fit\n",
    "import warnings\n",
    "\n",
    "def CO_AddNoise(y, tau = 1, amiMethod = 'even', extraParam = None, randomSeed = None):\n",
    "    \"\"\"\n",
    "    CO_AddNoise: Changes in the automutual information with the addition of noise\n",
    "\n",
    "    Parameters:\n",
    "    y (array-like): The input time series (should be z-scored)\n",
    "    tau (int or str): The time delay for computing AMI (default: 1)\n",
    "    amiMethod (str): The method for computing AMI:\n",
    "                      'std1','std2','quantiles','even' for histogram-based estimation,\n",
    "                      'gaussian','kernel','kraskov1','kraskov2' for estimation using JIDT\n",
    "    extraParam: e.g., the number of bins input to CO_HistogramAMI, or parameter for IN_AutoMutualInfo\n",
    "    randomSeed (int): Settings for resetting the random seed for reproducible results\n",
    "\n",
    "    Returns:\n",
    "    dict: Statistics on the resulting set of automutual information estimates\n",
    "    \"\"\"\n",
    "\n",
    "    if not BF_iszscored(y):\n",
    "        warnings.warn(\"Input time series should be z-scored\")\n",
    "    \n",
    "    # Set tau to minimum of autocorrelation function if 'ac' or 'tau'\n",
    "    if tau in ['ac', 'tau']:\n",
    "        tau = CO_FirstCrossing(y, 'ac', 0, 'discrete')\n",
    "    \n",
    "    # Generate noise\n",
    "    if randomSeed is not None:\n",
    "        np.random.seed(randomSeed)\n",
    "    noise = np.random.randn(len(y)) # generate uncorrelated additive noise\n",
    "\n",
    "    # Set up noise range\n",
    "    noiseRange = np.linspace(0, 3, 50) # compare properties across this noise range\n",
    "    numRepeats = len(noiseRange)\n",
    "\n",
    "    # Compute the automutual information across a range of noise levels\n",
    "    amis = np.zeros(numRepeats)\n",
    "    if amiMethod in ['std1', 'std2', 'quantiles', 'even']:\n",
    "        # histogram-based methods using my naive implementation in CO_Histogram\n",
    "        for i in range(numRepeats):\n",
    "            amis[i] = CO_HistogramAMI(y + noiseRange[i]*noise, tau, amiMethod, extraParam)\n",
    "            if np.isnan(amis[i]):\n",
    "                raise ValueError('Error computing AMI: Time series too short (?)')\n",
    "    if amiMethod in ['gaussian','kernel','kraskov1','kraskov2']:\n",
    "        for i in range(numRepeats):\n",
    "            amis[i] = IN_AutoMutualInfo(y + noiseRange[i]*noise, tau, amiMethod, extraParam)\n",
    "            if np.isnan(amis[i]):\n",
    "                raise ValueError('Error computing AMI: Time series too short (?)')\n",
    "    \n",
    "    # Output statistics\n",
    "    out = {}\n",
    "    # Proportion decreases\n",
    "    out['pdec'] = np.sum(np.diff(amis) < 0) / (numRepeats - 1)\n",
    "\n",
    "    # Mean change in AMI\n",
    "    out['meanch'] = np.mean(np.diff(amis))\n",
    "\n",
    "    # Autocorrelation of AMIs\n",
    "    out['ac1'] = CO_AutoCorr(amis, 1, 'Fourier')\n",
    "    out['ac2'] = CO_AutoCorr(amis, 2, 'Fourier')\n",
    "\n",
    "    # Noise level required to reduce ami to proportion x of its initial value\n",
    "    firstUnderVals = [0.75, 0.50, 0.25]\n",
    "    for val in firstUnderVals:\n",
    "        out[f'firstUnder{val*100}'] = firstUnder_fn(val * amis[0], noiseRange, amis)\n",
    "\n",
    "    # AMI at actual noise levels: 0.5, 1, 1.5 and 2\n",
    "    noiseLevels = [0.5, 1, 1.5, 2]\n",
    "    for nlvl in noiseLevels:\n",
    "        out[f'ami_at_{nlvl*10}'] = amis[np.argmax(noiseRange >= nlvl)]\n",
    "\n",
    "    # Count number of times the AMI function crosses its mean\n",
    "    out['pcrossmean'] = np.sum(np.diff(np.sign(amis - np.mean(amis))) != 0) / (numRepeats - 1)\n",
    "\n",
    "    # Fit exponential decay\n",
    "    expFunc = lambda x, a, b : a * np.exp(b * x)\n",
    "    popt, pcov = curve_fit(expFunc, noiseRange, amis, p0=[amis[0], -1])\n",
    "    out['fitexpa'], out['fitexpb'] = popt\n",
    "    residuals = amis - expFunc(noiseRange, *popt)\n",
    "    ss_res = np.sum(residuals**2)\n",
    "    ss_tot = np.sum((amis - np.mean(amis))**2)\n",
    "    out['fitexpr2'] = 1 - (ss_res / ss_tot)\n",
    "    out['fitexpadjr2'] = 1 - (1-out['fitexpr2'])*(len(amis)-1)/(len(amis)-2-1)\n",
    "    out['fitexprmse'] = np.sqrt(np.mean(residuals**2))\n",
    "\n",
    "    # Fit linear function\n",
    "    p = np.polyfit(noiseRange, amis, 1)\n",
    "    out['fitlina'], out['fitlinb'] = p\n",
    "    lin_fit = np.polyval(p, noiseRange)\n",
    "    out['linfit_mse'] = np.mean((lin_fit - amis)**2)\n",
    "\n",
    "    return out\n",
    "\n",
    "# helper functions\n",
    "def firstUnder_fn(x, m, p):\n",
    "    \"\"\"\n",
    "    Find the value of m for the first time p goes under the threshold, x. \n",
    "    p and m vectors of the same length\n",
    "    \"\"\"\n",
    "    first_i = next((m_val for m_val, p_val in zip(m, p) if p_val < x), m[-1])\n",
    "    return first_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z3/q9sgsjh15j1_0jw8qp3s0xn40000gn/T/ipykernel_76482/1518729724.py:29: UserWarning: Input time series should be z-scored\n",
      "  warnings.warn(\"Input time series should be z-scored\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mCO_AddNoise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 49\u001b[0m, in \u001b[0;36mCO_AddNoise\u001b[0;34m(y, tau, amiMethod, extraParam, randomSeed)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amiMethod \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquantiles\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meven\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# histogram-based methods using my naive implementation in CO_Histogram\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(numRepeats):\n\u001b[0;32m---> 49\u001b[0m         amis[i] \u001b[38;5;241m=\u001b[39m \u001b[43mCO_HistogramAMI\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnoiseRange\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamiMethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextraParam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(amis[i]):\n\u001b[1;32m     51\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError computing AMI: Time series too short (?)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/py-hctsa-project/Operations/CO_HistogramAMI.py:28\u001b[0m, in \u001b[0;36mCO_HistogramAMI\u001b[0;34m(y, tau, meth, numBins)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Bins for the data\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# same for both -- assume same distribution (true for stationary processes, or small lags)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meven\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 28\u001b[0m     b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(np\u001b[38;5;241m.\u001b[39mmin(y), np\u001b[38;5;241m.\u001b[39mmax(y), \u001b[43mnumBins\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Add increment buffer to ensure all points are included\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     inc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "CO_AddNoise(ts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1 = np.loadtxt(\"ts1.txt\")\n",
    "ts2 = np.loadtxt(\"ts2.txt\")\n",
    "ts3 = np.loadtxt(\"ts3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Operations.CO_FirstCrossing import CO_FirstCrossing\n",
    "\n",
    "def CO_HistogramAMI(y, tau = 1, meth = 'even', numBins = 10):\n",
    "    \"\"\"\n",
    "    CO_HistogramAMI: The automutual information of the distribution using histograms.\n",
    "\n",
    "    Parameters:\n",
    "    y (array-like): The input time series\n",
    "    tau (int, list or str): The time-lag(s) (default: 1)\n",
    "    meth (str): The method of computing automutual information:\n",
    "                'even': evenly-spaced bins through the range of the time series,\n",
    "                'std1', 'std2': bins that extend only up to a multiple of the\n",
    "                                standard deviation from the mean of the time series to exclude outliers,\n",
    "                'quantiles': equiprobable bins chosen using quantiles.\n",
    "    num_bins (int): The number of bins (default: 10)\n",
    "\n",
    "    Returns:\n",
    "    float or dict: The automutual information calculated in this way.\n",
    "    \"\"\"\n",
    "    # Use first zero crossing of the ACF as the time lag\n",
    "    if isinstance(tau, str) and tau in ['ac', 'tau']:\n",
    "        tau = CO_FirstCrossing(y, 'ac', 0, 'discrete')\n",
    "    \n",
    "    # Bins for the data\n",
    "    # same for both -- assume same distribution (true for stationary processes, or small lags)\n",
    "    if meth == 'even':\n",
    "        b = np.linspace(np.min(y), np.max(y), numBins + 1)\n",
    "        # Add increment buffer to ensure all points are included\n",
    "        inc = 0.1\n",
    "        b[0] -= inc\n",
    "        b[-1] += inc\n",
    "    elif meth == 'std1': # bins out to +/- 1 std\n",
    "        b = np.linspace(-1, 1, numBins + 1)\n",
    "        if np.min(y) < -1:\n",
    "            b = np.concatenate(([np.min(y) - 0.1], b))\n",
    "        if np.max(y) > 1:\n",
    "            b = np.concatenate((b, [np.max(y) + 0.1]))\n",
    "    elif meth == 'std2': # bins out to +/- 2 std\n",
    "        b = np.linspace(-2, 2, numBins + 1)\n",
    "        if np.min(y) < -2:\n",
    "            b = np.concatenate(([np.min(y) - 0.1], b))\n",
    "        if np.max(y) > 2:\n",
    "            b = np.concatenate((b, [np.max(y) + 0.1]))\n",
    "    elif meth == 'quantiles': # use quantiles with ~equal number in each bin\n",
    "        b = np.quantile(y, np.linspace(0, 1, numBins + 1))\n",
    "        b[0] -= 0.1\n",
    "        b[-1] += 0.1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method '{meth}'\")\n",
    "    \n",
    "    # Sometimes bins can be added (e.g., with std1 and std2), so need to redefine numBins\n",
    "    numBins = len(b) - 1\n",
    "\n",
    "    # Form the time-delay vectors y1 and y2\n",
    "    if not isinstance(tau, (list, np.ndarray)):\n",
    "        # if only single time delay as integer, make into a one element list\n",
    "        tau = [tau]\n",
    "\n",
    "    amis = np.zeros(len(tau))\n",
    "\n",
    "    for i, t in enumerate(tau):\n",
    "        y1 = y[:-t]\n",
    "        y2 = y[t:]\n",
    "\n",
    "        # Joint distribution of y1 and y2\n",
    "        pij, _, _ = np.histogram2d(y1, y2, bins=(b, b))\n",
    "        pij = pij[:numBins, :numBins]  # joint\n",
    "        pij = pij / np.sum(pij)  # normalize\n",
    "        pi = np.sum(pij, axis=1)  # marginal\n",
    "        pj = np.sum(pij, axis=0)  # other marginal\n",
    "\n",
    "        pii = np.tile(pi, (numBins, 1)).T\n",
    "        pjj = np.tile(pj, (numBins, 1))\n",
    "\n",
    "        r = pij > 0  # Defining the range in this way, we set log(0) = 0\n",
    "        amis[i] = np.sum(pij[r] * np.log(pij[r] / pii[r] / pjj[r]))\n",
    "\n",
    "    if len(tau) == 1:\n",
    "        return amis[0]\n",
    "    else:\n",
    "        return {f'ami{i+1}': ami for i, ami in enumerate(amis)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04761744653583484"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CO_HistogramAMI(ts3, 'ac', meth='quantiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0797345972733234"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.kurtosis(ts3_zs, fisher=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepBinary(X):\n",
    "    # Transform real values to 0 if <=0 and 1 if >0:\n",
    "    Y = np.zeros(len(X))\n",
    "    Y[X > 0] = 1\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "expFunc = lambda x, a, b : a * np.exp(b * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def firstUnder_fn(x, m, p):\n",
    "    \"\"\"\n",
    "    Find the value of m for the first time p goes under the threshold, x. \n",
    "    p and m vectors of the same length\n",
    "    \"\"\"\n",
    "    first_i = next((m_val for m_val, p_val in zip(m, p) if p_val < x), m[-1])\n",
    "    return first_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BF_Binarize(y, binarizeHow='diff'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if binarizeHow == 'diff':\n",
    "        # Binary signal: 1 for stepwise increases, 0 for stepwise decreases\n",
    "        yBin = stepBinary(np.diff(y))\n",
    "    \n",
    "    elif binarizeHow == 'mean':\n",
    "        # Binary signal: 1 for above mean, 0 for below mean\n",
    "        yBin = stepBinary(y - np.mean(y))\n",
    "    \n",
    "    elif binarizeHow == 'median':\n",
    "        # Binary signal: 1 for above median, 0 for below median\n",
    "        yBin = stepBinary(y - np.median(y))\n",
    "    \n",
    "    elif binarizeHow == 'iqr':\n",
    "        # Binary signal: 1 if inside interquartile range, 0 otherwise\n",
    "        iqr = np.quantile(y,[.25,.75])\n",
    "        iniqr = np.logical_and(y > iqr[0], y<iqr[1])\n",
    "        yBin = np.zeros(len(y))\n",
    "        yBin[iniqr] = 1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown binary transformation setting '{binarizeHow}'\")\n",
    "\n",
    "    return yBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mea = BF_Binarize(ts1, binarizeHow='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.random.randn(3, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.size(mat, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BF_SignChange(y, doFind=0):\n",
    "    \"\"\"\n",
    "    Where a data vector changes sign.\n",
    "\n",
    "    \"\"\"\n",
    "    if doFind == 0:\n",
    "        return (np.multiply(y[1:],y[0:len(y)-1]) < 0)\n",
    "    indexs = np.where((np.multiply(y[1:],y[0:len(y)-1]) < 0))\n",
    "\n",
    "    return indexs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = BF_SignChange(ts1, doFind=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 14,  30,  46,  61,  77,  93, 108, 124, 140, 156, 171, 187, 203,\n",
       "        218, 234, 250, 266, 281, 297, 313, 328, 344, 360, 375, 391, 407,\n",
       "        423, 438, 454, 470, 485, 501, 517, 533, 548, 564, 580, 595, 611,\n",
       "        627, 643, 658, 674, 690, 705, 721, 737, 752, 768, 784, 800, 815,\n",
       "        831, 847, 862, 878, 894, 910, 925, 941, 957, 972, 988]),)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import moment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DN_Moments(y, theMom):\n",
    "    \"\"\"\n",
    "    A moment of the distribution of the input time series.\n",
    "    \n",
    "    \"\"\"\n",
    "    out = moment(y, theMom) / np.std(y) # normalized\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.005128976116081682"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_Moments(ts1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def DN_Mean(y, mean_type='arithmetic'):\n",
    "    \"\"\"\n",
    "    A given measure of location of a data vector.\n",
    "\n",
    "    Parameters:\n",
    "    y (array-like): The input data vector\n",
    "    mean_type (str): The type of mean to calculate\n",
    "        'norm' or 'arithmetic': arithmetic mean\n",
    "        'median': median\n",
    "        'geom': geometric mean\n",
    "        'harm': harmonic mean\n",
    "        'rms': root-mean-square\n",
    "        'iqm': interquartile mean\n",
    "        'midhinge': midhinge\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated mean value\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If an unknown mean type is specified\n",
    "    \"\"\"\n",
    "    y = np.array(y)\n",
    "    N = len(y)\n",
    "\n",
    "    if mean_type in ['norm', 'arithmetic']:\n",
    "        return np.mean(y)\n",
    "    elif mean_type == 'median':\n",
    "        return np.median(y)\n",
    "    elif mean_type == 'geom':\n",
    "        return stats.gmean(y)\n",
    "    elif mean_type == 'harm':\n",
    "        return stats.hmean(y)\n",
    "    elif mean_type == 'rms':\n",
    "        return np.sqrt(np.mean(y**2))\n",
    "    elif mean_type == 'iqm':\n",
    "        p = np.percentile(y, [25, 75])\n",
    "        return np.mean(y[(y >= p[0]) & (y <= p[1])])\n",
    "    elif mean_type == 'midhinge':\n",
    "        p = np.percentile(y, [25, 75])\n",
    "        return np.mean(p)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mean type '{mean_type}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DN_ProportionValues(x, propWhat='positive'):\n",
    "\n",
    "    N = len(x)\n",
    "\n",
    "    if propWhat == 'zeros':\n",
    "        # returns the proportion of zeros in the input vector\n",
    "        out = sum(x == 0) / N\n",
    "    elif propWhat == 'positive':\n",
    "        out = sum(x > 0) / N\n",
    "    elif propWhat == 'geq0':\n",
    "        out = sum(x >= 0) / N\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown condition to measure: {propWhat}\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.519"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_ProportionValues(ts3, 'geq0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DN_Quantile(y, p=0.5):\n",
    "    \"\"\"\n",
    "    Calculates the quantile value at a specified proportion, p.\n",
    "\n",
    "    Parameters:\n",
    "    y (array-like): The input data vector\n",
    "    p (float): The quantile proportion (default is 0.5, which is the median)\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated quantile value\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If p is not a number between 0 and 1\n",
    "    \"\"\"\n",
    "    if p == 0.5:\n",
    "        print(\"Using quantile p = 0.5 (median) by default\")\n",
    "    \n",
    "    if not isinstance(p, (int, float)) or p < 0 or p > 1:\n",
    "        raise ValueError(\"p must specify a proportion, in (0,1)\")\n",
    "    \n",
    "    return np.quantile(y, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.58822"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_Quantile(ts1, p=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform, norm, geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'geom_gen' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgeom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m(ts2)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'geom_gen' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "geom.fit(ts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EN_CID(y):\n",
    "    \"\"\"\n",
    "    Simple complexity measure of a time series.\n",
    "\n",
    "    Estimates of 'complexity' of a time series as the stretched-out length of the\n",
    "    lines resulting from a line-graph of the time series.\n",
    "\n",
    "    Parameters:\n",
    "    y (array-like): the input time series\n",
    "\n",
    "    Returns:\n",
    "    out (dict): \n",
    "    \"\"\"\n",
    "    CE1 = f_CE1(y)\n",
    "    CE2 = f_CE2(y)\n",
    "\n",
    "    minCE1 = f_CE1(np.sort(y))\n",
    "    minCE2 = f_CE2(np.sort(y))\n",
    "\n",
    "    CE1_norm = CE1 / minCE1\n",
    "    CE2_norm = CE2 / minCE2\n",
    "\n",
    "    out = {'CE1':CE1,'CE2':CE2,'minCE1':minCE1,'minCE2':minCE2,\n",
    "            'CE1_norm':CE1_norm,'CE2_norm':CE2_norm}\n",
    "\n",
    "    return out\n",
    "\n",
    "def f_CE1(y):\n",
    "    return np.sqrt(np.mean(np.power(np.diff(y),2)))\n",
    "\n",
    "def f_CE2(y):\n",
    "    return np.mean(np.sqrt(1 + np.power(np.diff(y),2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CE1': 1.4397258123179755,\n",
       " 'CE2': 1.623039534424403,\n",
       " 'minCE1': 0.028789278256714637,\n",
       " 'minCE2': 1.0003926815512982,\n",
       " 'CE1_norm': 50.00909711872275,\n",
       " 'CE2_norm': 1.6224024469147185}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EN_CID(ts3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DN_Spread(y, spreadMeasure='std'):\n",
    "    \"\"\"\n",
    "    Measure of spread of the input time series.\n",
    "    Returns the spread of the raw data vector, as the standard deviation,\n",
    "    inter-quartile range, mean absolute deviation, or median absolute deviation.\n",
    "    \"\"\"\n",
    "    if spreadMeasure == 'std':\n",
    "        out = np.std(y)\n",
    "    elif spreadMeasure == 'iqr':\n",
    "        out = stats.iqr(y)\n",
    "    elif spreadMeasure == 'mad':\n",
    "        out = mad(y)\n",
    "    elif spreadMeasure == 'mead':\n",
    "        out = mead(y)\n",
    "    else:\n",
    "        raise ValueError('spreadMeasure must be one of std, iqr, mad or mead')\n",
    "\n",
    "    return out\n",
    "\n",
    "def mad(data, axis=None):\n",
    "    return np.mean(np.absolute(data - np.mean(data, axis)), axis)\n",
    "\n",
    "def mead(data, axis=None):\n",
    "    return np.median(np.absolute(data - np.median(data, axis)), axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66015"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_Spread(ts2, spreadMeasure='mead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DN_Unique(x):\n",
    "    \"\"\"\n",
    "    The proportion of the time series that are unique values.\n",
    "\n",
    "    Parameters:\n",
    "    x (array-like): the input data vector\n",
    "\n",
    "    Returns:\n",
    "    out (float): the proportion of time series that are unique values\n",
    "    \"\"\"\n",
    "\n",
    "    return len(np.unique(x)) / len(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.977"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_Unique(ts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "897"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(ts1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CO_NonlinearAutocorr(y,taus,doAbs ='empty'):\n",
    "\n",
    "    if doAbs == 'empty':\n",
    "\n",
    "        if len(taus) % 2 == 1:\n",
    "\n",
    "            doAbs = 0\n",
    "\n",
    "        else:\n",
    "\n",
    "            doAbs = 1\n",
    "\n",
    "    N = len(y)\n",
    "    tmax = np.max(taus)\n",
    "\n",
    "    nlac = y[tmax:N]\n",
    "\n",
    "    for i in taus:\n",
    "\n",
    "        nlac = np.multiply(nlac,y[ tmax - i:N - i ])\n",
    "\n",
    "    if doAbs:\n",
    "\n",
    "        return np.mean(np.absolute(nlac))\n",
    "\n",
    "    else:\n",
    "\n",
    "        return np.mean(nlac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32868826608330426"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CO_NonlinearAutocorr(ts1, [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import trim_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DN_TrimmedMean(y, n=0):\n",
    "    \"\"\"\n",
    "    Mean of the trimmed time series using trimmean.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    y (array-like): the input time series\n",
    "    n (float): the fraction of highest and lowest values in y to exclude from the mean calculation\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    out (float): the mean of the trimmed time series.\n",
    "    \"\"\"\n",
    "    n *= 0.01\n",
    "    N = len(y)\n",
    "    trim = int(np.round(N * n / 2))\n",
    "    y = np.sort(y)\n",
    "\n",
    "    out = np.mean(y[trim:N-trim])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0023594444444444205"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_TrimmedMean(ts1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DN_Burstiness(y):\n",
    "    \"\"\"\n",
    "    Calculate the burstiness statistic of a time series.\n",
    "\n",
    "    This function returns the 'burstiness' statistic as defined in\n",
    "    Goh and Barabasi's paper, \"Burstiness and memory in complex systems,\"\n",
    "    Europhys. Lett. 81, 48002 (2008).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array-like\n",
    "        The input time series.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The original burstiness statistic, B, and the improved\n",
    "        burstiness statistic, B_Kim.\n",
    "    \"\"\"\n",
    "    \n",
    "    mean = np.mean(y)\n",
    "    std = np.std(y)\n",
    "\n",
    "    r = np.divide(std,mean) # coefficient of variation\n",
    "    B = np.divide((r - 1), (r + 1)) # Original Goh and Barabasi burstiness statistic, B\n",
    "\n",
    "    # improved burstiness statistic, accounting for scaling for finite time series\n",
    "    # Kim and Jo, 2016, http://arxiv.org/pdf/1604.01125v1.pdf\n",
    "    N = len(y)\n",
    "    p1 = np.sqrt(N+1)*r - np.sqrt(N-1)\n",
    "    p2 = (np.sqrt(N+1)-2)*r + np.sqrt(N-1)\n",
    "\n",
    "    B_Kim = np.divide(p1, p2)\n",
    "\n",
    "    out = {'B': B, 'B_Kim': B_Kim}\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B': 0.92662389689055, 'B_Kim': 0.9867869006600554}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_Burstiness(ts3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Operations.CO_HistogramAMI import CO_HistogramAMI\n",
    "from Operations.CO_FirstCrossing import CO_FirstCrossing\n",
    "from Operations.IN_AutoMutualInfo import IN_AutoMutualInfo\n",
    "from Operations.CO_AutoCorr import CO_AutoCorr\n",
    "from PeripheryFunctions.BF_SignChange import BF_SignChange\n",
    "from PeripheryFunctions.BF_iszscored import BF_iszscored\n",
    "from scipy.optimize import curve_fit\n",
    "import warnings\n",
    "\n",
    "def CO_AddNoise(y, tau = 1, amiMethod = 'even', extraParam = None, randomSeed = None):\n",
    "    \"\"\"\n",
    "    CO_AddNoise: Changes in the automutual information with the addition of noise\n",
    "\n",
    "    Parameters:\n",
    "    y (array-like): The input time series (should be z-scored)\n",
    "    tau (int or str): The time delay for computing AMI (default: 1)\n",
    "    amiMethod (str): The method for computing AMI:\n",
    "                      'std1','std2','quantiles','even' for histogram-based estimation,\n",
    "                      'gaussian','kernel','kraskov1','kraskov2' for estimation using JIDT\n",
    "    extraParam: e.g., the number of bins input to CO_HistogramAMI, or parameter for IN_AutoMutualInfo\n",
    "    randomSeed (int): Settings for resetting the random seed for reproducible results\n",
    "\n",
    "    Returns:\n",
    "    dict: Statistics on the resulting set of automutual information estimates\n",
    "    \"\"\"\n",
    "\n",
    "    if not BF_iszscored(y):\n",
    "        warnings.warn(\"Input time series should be z-scored\")\n",
    "    \n",
    "    # Set tau to minimum of autocorrelation function if 'ac' or 'tau'\n",
    "    if tau in ['ac', 'tau']:\n",
    "        tau = CO_FirstCrossing(y, 'ac', 0, 'discrete')\n",
    "    \n",
    "    # Generate noise\n",
    "    if randomSeed is not None:\n",
    "        np.random.seed(randomSeed)\n",
    "    noise = np.random.randn(len(y)) # generate uncorrelated additive noise\n",
    "\n",
    "    # Set up noise range\n",
    "    noiseRange = np.linspace(0, 3, 50) # compare properties across this noise range\n",
    "    numRepeats = len(noiseRange)\n",
    "\n",
    "    # Compute the automutual information across a range of noise levels\n",
    "    amis = np.zeros(numRepeats)\n",
    "    if amiMethod in ['std1', 'std2', 'quantiles', 'even']:\n",
    "        # histogram-based methods using my naive implementation in CO_Histogram\n",
    "        for i in range(numRepeats):\n",
    "            # use default num of bins for CO_HistogramAMI if not specified\n",
    "            amis[i] = CO_HistogramAMI(y + noiseRange[i]*noise, tau, amiMethod, extraParam or 10)\n",
    "            if np.isnan(amis[i]):\n",
    "                raise ValueError('Error computing AMI: Time series too short (?)')\n",
    "    if amiMethod in ['gaussian','kernel','kraskov1','kraskov2']:\n",
    "        for i in range(numRepeats):\n",
    "            amis[i] = IN_AutoMutualInfo(y + noiseRange[i]*noise, tau, amiMethod, extraParam)\n",
    "            if np.isnan(amis[i]):\n",
    "                raise ValueError('Error computing AMI: Time series too short (?)')\n",
    "    \n",
    "    # Output statistics\n",
    "    out = {}\n",
    "    # Proportion decreases\n",
    "    out['pdec'] = np.sum(np.diff(amis) < 0) / (numRepeats - 1)\n",
    "\n",
    "    # Mean change in AMI\n",
    "    out['meanch'] = np.mean(np.diff(amis))\n",
    "\n",
    "    # Autocorrelation of AMIs\n",
    "    out['ac1'] = CO_AutoCorr(amis, 1, 'Fourier')[0]\n",
    "    out['ac2'] = CO_AutoCorr(amis, 2, 'Fourier')[0]\n",
    "\n",
    "    # Noise level required to reduce ami to proportion x of its initial value\n",
    "    firstUnderVals = [0.75, 0.50, 0.25]\n",
    "    for val in firstUnderVals:\n",
    "        out[f'firstUnder{val*100}'] = firstUnder_fn(val * amis[0], noiseRange, amis)\n",
    "\n",
    "    # AMI at actual noise levels: 0.5, 1, 1.5 and 2\n",
    "    noiseLevels = [0.5, 1, 1.5, 2]\n",
    "    for nlvl in noiseLevels:\n",
    "        out[f'ami_at_{int(nlvl*10)}'] = amis[np.argmax(noiseRange >= nlvl)]\n",
    "\n",
    "    # Count number of times the AMI function crosses its mean\n",
    "    out['pcrossmean'] = np.sum(np.diff(np.sign(amis - np.mean(amis))) != 0) / (numRepeats - 1)\n",
    "\n",
    "    # Fit exponential decay\n",
    "    expFunc = lambda x, a, b : a * np.exp(b * x)\n",
    "    popt, pcov = curve_fit(expFunc, noiseRange, amis, p0=[amis[0], -1])\n",
    "    out['fitexpa'], out['fitexpb'] = popt\n",
    "    residuals = amis - expFunc(noiseRange, *popt)\n",
    "    ss_res = np.sum(residuals**2)\n",
    "    ss_tot = np.sum((amis - np.mean(amis))**2)\n",
    "    out['fitexpr2'] = 1 - (ss_res / ss_tot)\n",
    "    out['fitexpadjr2'] = 1 - (1-out['fitexpr2'])*(len(amis)-1)/(len(amis)-2-1)\n",
    "    out['fitexprmse'] = np.sqrt(np.mean(residuals**2))\n",
    "\n",
    "    # Fit linear function\n",
    "    p = np.polyfit(noiseRange, amis, 1)\n",
    "    out['fitlina'], out['fitlinb'] = p\n",
    "    lin_fit = np.polyval(p, noiseRange)\n",
    "    out['linfit_mse'] = np.mean((lin_fit - amis)**2)\n",
    "\n",
    "    return out\n",
    "\n",
    "# helper functions\n",
    "def firstUnder_fn(x, m, p):\n",
    "    \"\"\"\n",
    "    Find the value of m for the first time p goes under the threshold, x. \n",
    "    p and m vectors of the same length\n",
    "    \"\"\"\n",
    "    first_i = next((m_val for m_val, p_val in zip(m, p) if p_val < x), m[-1])\n",
    "    return first_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.optimize import curve_fit\n",
    "from Operations.CO_AutoCorr import CO_AutoCorr\n",
    "from Operations.CO_FirstCrossing import CO_FirstCrossing\n",
    "from PeripheryFunctions.BF_SignChange import BF_SignChange\n",
    "import warnings\n",
    "\n",
    "def CO_AutoCorrShape(y, stopWhen = 'posDrown'):\n",
    "    \"\"\"\n",
    "    CO_AutoCorrShape: How the autocorrelation function changes with the time lag.\n",
    "\n",
    "    Outputs include the number of peaks, and autocorrelation in the\n",
    "    autocorrelation function (ACF) itself.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y : array_like\n",
    "        The input time series\n",
    "    stopWhen : str or int, optional\n",
    "        The criterion for the maximum lag to measure the ACF up to.\n",
    "        Default is 'posDrown'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary containing various metrics about the autocorrelation function.\n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "\n",
    "    # Only look up to when two consecutive values are under the significance threshold\n",
    "    th = 2 / np.sqrt(N)  # significance threshold\n",
    "\n",
    "    # Calculate the autocorrelation function, up to a maximum lag, length of time series (hopefully it's cropped by then)\n",
    "    acf = []\n",
    "\n",
    "    # At what lag does the acf drop to zero, Ndrown (by my definition)?\n",
    "    if isinstance(stopWhen, int):\n",
    "        taus = list(range(0, stopWhen+1))\n",
    "        acf = CO_AutoCorr(y, taus, 'Fourier')\n",
    "        Ndrown = stopWhen\n",
    "    elif stopWhen in ['posDrown', 'drown', 'doubleDrown']:\n",
    "        # Compute ACF up to a given threshold:\n",
    "        Ndrown = 0 # the point at which ACF ~ 0\n",
    "        if stopWhen == 'posDrown':\n",
    "            # stop when ACF drops below threshold, th\n",
    "            for i in range(1, N+1):\n",
    "                acf_val = CO_AutoCorr(y, i-1, 'Fourier')[0]\n",
    "                if np.isnan(acf_val):\n",
    "                    warnings.warn(\"Weird time series (constant?)\")\n",
    "                    out = np.nan\n",
    "                if acf_val < th:\n",
    "                    # Ensure ACF is all positive\n",
    "                    if acf_val > 0:\n",
    "                        Ndrown = i\n",
    "                        acf.append(acf_val)\n",
    "                    else:\n",
    "                        # stop at the previous point if not positive\n",
    "                        Ndrown = i-1\n",
    "                    # ACF has dropped below threshold, break the for loop...\n",
    "                    break\n",
    "                # hasn't dropped below thresh, append to list \n",
    "                acf.append(acf_val)\n",
    "            # This should yield the initial, positive portion of the ACF.\n",
    "            assert all(np.array(acf) > 0)\n",
    "        elif stopWhen == 'drown':\n",
    "            # Stop when ACF is very close to 0 (within threshold, th = 2/sqrt(N))\n",
    "            for i in range(1, N+1):\n",
    "                acf_val = CO_AutoCorr(y, i-1, 'Fourier')[0] # acf vector indicies are not lags\n",
    "                # if positive and less than thresh\n",
    "                if i > 0 and abs(acf_val) < th:\n",
    "                    Ndrown = i\n",
    "                    acf.append(acf_val)\n",
    "                    break\n",
    "                acf.append(acf_val)\n",
    "        elif stopWhen == 'doubleDrown':\n",
    "            # Stop at 2*tau, where tau is the lag where ACF ~ 0 (within 1/sqrt(N) threshold)\n",
    "            for i in range(1, N+1):\n",
    "                acf_val = CO_AutoCorr(y, i-1, 'Fourier')[0]\n",
    "                if Ndrown > 0 and i == Ndrown * 2:\n",
    "                    acf.append(acf_val)\n",
    "                    break\n",
    "                elif i > 1 and abs(acf_val) < th:\n",
    "                    Ndrown = i\n",
    "                acf.append(acf_val)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown ACF decay criterion: '{stopWhen}'\")\n",
    "\n",
    "    acf = np.array(acf)\n",
    "    Nac = len(acf)\n",
    "\n",
    "    # Check for good behavior\n",
    "    if np.any(np.isnan(acf)):\n",
    "        # This is an anomalous time series (e.g., all constant, or conatining NaNs)\n",
    "        out = np.NaN\n",
    "    \n",
    "    out = {}\n",
    "    out['Nac'] = Ndrown\n",
    "\n",
    "    # Basic stats on the ACF\n",
    "    out['sumacf'] = np.sum(acf)\n",
    "    out['meanacf'] = np.mean(acf)\n",
    "    if stopWhen != 'posDrown':\n",
    "        out['meanabsacf'] = np.mean(np.abs(acf))\n",
    "        out['sumabsacf'] = np.sum(np.abs(acf))\n",
    "\n",
    "    # Autocorrelation of the ACF\n",
    "    minPointsForACFofACF = 5 # can't take lots of complex stats with fewer than this\n",
    "    if Nac > minPointsForACFofACF:\n",
    "        out['ac1'] = CO_AutoCorr(acf, 1, 'Fourier')[0]\n",
    "        if all(acf > 0):\n",
    "            out['actau'] = np.nan\n",
    "        else:\n",
    "            out['actau'] = CO_AutoCorr(acf, CO_FirstCrossing(acf, 'ac', 0, 'discrete'), 'Fourier')[0]\n",
    "    else:\n",
    "        out['ac1'] = np.nan\n",
    "        out['actau'] = np.nan\n",
    "    \n",
    "    # Local extrema\n",
    "    dacf = np.diff(acf)\n",
    "    ddacf = np.diff(dacf)\n",
    "    extrr = BF_SignChange(dacf, 1)\n",
    "    sdsp = ddacf[extrr]\n",
    "\n",
    "    # Proportion of local minima\n",
    "    out['nminima'] = np.sum(sdsp > 0)\n",
    "    out['meanminima'] = np.mean(sdsp[sdsp > 0])\n",
    "\n",
    "    # Proportion of local maxima\n",
    "    out['nmaxima'] = np.sum(sdsp < 0)\n",
    "    out['meanmaxima'] = abs(np.mean(sdsp[sdsp < 0])) # must be negative: make it positive\n",
    "\n",
    "    # Proportion of extrema\n",
    "    out['nextrema'] = len(sdsp)\n",
    "    out['pextrema'] = len(sdsp) / Nac\n",
    "\n",
    "    # Fit exponential decay (only for 'posDrown', and if there are enough points)\n",
    "    # Should probably only do this up to the first zero crossing...\n",
    "    fitSuccess = False\n",
    "    minPointsToFitExp = 4 # (need at least four points to fit exponential)\n",
    "    if stopWhen == 'posDrown' and Nac >= minPointsToFitExp:\n",
    "        # Fit exponential decay to (absolute) ACF:\n",
    "        # (kind of only makes sense for the first positive period)\n",
    "        expFunc = lambda x, b : np.exp(-b * x)\n",
    "        try:\n",
    "            popt, _ = curve_fit(expFunc, np.arange(Nac), acf, p0=0.5)\n",
    "            fitSuccess = True\n",
    "        except:\n",
    "            fitSuccess = False\n",
    "    if fitSuccess:\n",
    "        bFit = popt[0] # fitted b\n",
    "        out['decayTimescale'] = 1 / bFit\n",
    "        expFit = expFunc(np.arange(Nac), bFit)\n",
    "        residuals = acf - expFit\n",
    "        out['fexpacf_r2'] = 1 - (np.sum(residuals**2) / np.sum((acf - np.mean(acf))**2))\n",
    "        # had to fit a second exponential function with negative b to get same output as MATLAB for std residuals\n",
    "        expFit2 = expFunc(np.arange(Nac), -bFit)\n",
    "        residuals2 = acf - expFit2\n",
    "        out['fexpacf_stdres'] = np.std(residuals2, ddof=1) # IMPORTANT *** DDOF=1 TO MATCH MATLAB STD ***\n",
    "    else:\n",
    "        # Fit inappropriate (or failed): return NaNs for the relevant stats\n",
    "        out['decayTimescale'] = np.nan\n",
    "        out['fexpacf_r2'] = np.nan\n",
    "        out['fexpacf_stdres'] = np.nan\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Nac': 14,\n",
       " 'sumacf': 1.745616526173292,\n",
       " 'meanacf': 0.11637443507821947,\n",
       " 'meanabsacf': 0.6154901825161944,\n",
       " 'sumabsacf': 9.232352737742916,\n",
       " 'ac1': 0.8415798606539007,\n",
       " 'actau': -0.1164707090370352,\n",
       " 'nminima': 0,\n",
       " 'meanminima': nan,\n",
       " 'nmaxima': 0,\n",
       " 'meanmaxima': nan,\n",
       " 'nextrema': 0,\n",
       " 'pextrema': 0.0,\n",
       " 'decayTimescale': nan,\n",
       " 'fexpacf_r2': nan,\n",
       " 'fexpacf_stdres': nan}"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CO_AutoCorrShape(ts1, stopWhen=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def BF_MutualInformation(v1, v2, r1 = 'range', r2 = 'range', numBins = 10):\n",
    "    \"\"\"\n",
    "    Compute mutual information between two data vectors using bin counting.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "        v1 (array-like): The first input vector\n",
    "        v2 (array-like): The second input vector\n",
    "        r1 (str or list): The bin-partitioning method for v1 ('range', 'quantile', or [min, max])\n",
    "        r2 (str or list): The bin-partitioning method for v2 ('range', 'quantile', or [min, max])\n",
    "        numBins (int): The number of bins to partition each vector into (default : 10)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        float: The mutual information computed between v1 and v2\n",
    "    \"\"\"\n",
    "    v1 = np.asarray(v1).flatten()\n",
    "    v2 = np.asarray(v2).flatten()\n",
    "\n",
    "    if len(v1) != len(v2):\n",
    "        raise ValueError(\"Input vectors must be the same length\")\n",
    "\n",
    "    N = len(v1)\n",
    "\n",
    "    # Create histograms\n",
    "    edges_i = SUB_GiveMeEdges(r1, v1, numBins)\n",
    "    edges_j = SUB_GiveMeEdges(r2, v2, numBins)\n",
    "\n",
    "    ni, _ = np.histogram(v1, edges_i)\n",
    "    nj, _ = np.histogram(v2, edges_j)\n",
    "\n",
    "    # Create a joint histogram\n",
    "    hist_xy, _, _ = np.histogram2d(v1, v2, [edges_i, edges_j])\n",
    "\n",
    "    # Normalize counts to probabilities\n",
    "    p_i = ni[:numBins] / N\n",
    "    p_j = nj[:numBins] / N\n",
    "    p_ij = hist_xy / N\n",
    "    p_ixp_j = np.outer(p_i, p_j)\n",
    "\n",
    "    # Calculate mutual information\n",
    "    mask = (p_ixp_j > 0) & (p_ij > 0)\n",
    "    if np.any(mask):\n",
    "        mi = np.sum(p_ij[mask] * np.log(p_ij[mask] / p_ixp_j[mask]))\n",
    "    else:\n",
    "        print(\"The histograms aren't catching any points. Perhaps due to an inappropriate custom range for binning the data.\")\n",
    "        mi = np.nan\n",
    "\n",
    "    return mi\n",
    "\n",
    "def SUB_GiveMeEdges(r, v, nbins):\n",
    "    EE = 1E-6 # this small addition gets lost in the last bin\n",
    "    if r == 'range':\n",
    "            return np.linspace(np.min(v), np.max(v) + EE, nbins + 1)\n",
    "    elif r == 'quantile': # bin edges based on quantiles\n",
    "        edges = np.quantile(v, np.linspace(0, 1, nbins + 1))\n",
    "        edges[-1] += EE\n",
    "        return edges\n",
    "    elif isinstance(r, (list, np.ndarray)) and len(r) == 2: # a two-component vector\n",
    "        return np.linspace(r[0], r[1] + EE, nbins + 1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown partitioning method '{r}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18659001559071042"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BF_MutualInformation(ts1, ts3, 'quantile', 'quantile', numBins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Operations.CO_AutoCorr import CO_AutoCorr\n",
    "from Operations.IN_AutoMutualInfo import IN_AutoMutualInfo\n",
    "from PeripheryFunctions.BF_MutualInformation import BF_MutualInformation\n",
    "import warnings\n",
    "\n",
    "def CO_FirstMin2(y, minWhat = 'mi-gaussian', extraParam = None, minNotMax = True):\n",
    "    \"\"\"\n",
    "    Time of first minimum in a given self-correlation function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array-like\n",
    "        The input time series.\n",
    "    minWhat : str, optional\n",
    "        The type of correlation to minimize. Options are 'ac' for autocorrelation,\n",
    "        or 'mi' for automutual information. By default, 'mi' specifies the\n",
    "        'gaussian' method from the Information Dynamics Toolkit. Other options\n",
    "        include 'mi-kernel', 'mi-kraskov1', 'mi-kraskov2' (from Information Dynamics Toolkit),\n",
    "        or 'mi-hist' (histogram-based method). Default is 'mi'.\n",
    "    extraParam : any, optional\n",
    "        An additional parameter required for the specified `minWhat` method (e.g., for Kraskov).\n",
    "    minNotMax : bool, optional\n",
    "        If True, return the maximum instead of the minimum. Default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The time of the first minimum (or maximum if `minNotMax` is True).\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(y)\n",
    "\n",
    "    # Define the autocorrelation function\n",
    "    if minWhat in ['ac', 'corr']:\n",
    "        # Autocorrelation implemented as CO_AutoCorr\n",
    "        corrfn = lambda x : CO_AutoCorr(y, tau=x, method='Fourier')\n",
    "    elif minWhat == 'mi-hist':\n",
    "        # if extraParam is none, use default num of bins in BF_MutualInformation (default : 10)\n",
    "        corrfn = lambda x : BF_MutualInformation(y[:-x], y[x:], 'range', 'range', extraParam or 10)\n",
    "    elif minWhat == 'mi-kraskov2':\n",
    "        # (using Information Dynamics Toolkit)\n",
    "        # extraParam is the number of nearest neighbors\n",
    "        corrfn = lambda x : IN_AutoMutualInfo(y, x, 'kraskov2', extraParam)\n",
    "    elif minWhat == 'mi-kraskov1':\n",
    "        # (using Information Dynamics Toolkit)\n",
    "        corrfn = lambda x : IN_AutoMutualInfo(y, x, 'kraskov1', extraParam)\n",
    "    elif minWhat == 'mi-kernel':\n",
    "        corrfn = lambda x : IN_AutoMutualInfo(y, x, 'kernel', extraParam)\n",
    "    elif minWhat in ['mi', 'mi-gaussian']:\n",
    "        corrfn = lambda x : IN_AutoMutualInfo(y, x, 'gaussian', extraParam)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown correlation type specified: {minWhat}\")\n",
    "    \n",
    "    # search for a minimum (incrementally through time lags until a minimum is found)\n",
    "    autoCorr = np.zeros(N-1) # pre-allocate maximum length autocorrelation vector\n",
    "    if minNotMax:\n",
    "        # FIRST LOCAL MINUMUM \n",
    "        for i in range(1, N):\n",
    "            autoCorr[i-1] = corrfn(i)\n",
    "            # Hit a NaN before got to a minimum -- there is no minimum\n",
    "            if np.isnan(autoCorr[i-1]):\n",
    "                warnings.warn(f\"No minimum in {minWhat} [[time series too short to find it?]]\")\n",
    "                out = np.nan\n",
    "            \n",
    "            # we're at a local minimum\n",
    "            if (i == 2) and (autoCorr[1] > autoCorr[0]):\n",
    "                # already increases at lag of 2 from lag of 1: a minimum (since ac(0) is maximal)\n",
    "                return 1\n",
    "            elif (i > 2) and autoCorr[i-3] > autoCorr[i-2] < autoCorr[i-1]:\n",
    "                # minimum at previous i\n",
    "                return i-1 # I found the first minimum!\n",
    "    else:\n",
    "        # FIRST LOCAL MAXIMUM\n",
    "        for i in range(1, N):\n",
    "            autoCorr[i-1] = corrfn(i)\n",
    "            # Hit a NaN before got to a max -- there is no max\n",
    "            if np.isnan(autoCorr[i-1]):\n",
    "                warnings.warn(f\"No minimum in {minWhat} [[time series too short to find it?]]\")\n",
    "                return np.nan\n",
    "\n",
    "            # we're at a local maximum\n",
    "            if i > 2 and autoCorr[i-3] < autoCorr[i-2] > autoCorr[i-1]:\n",
    "                return i-1\n",
    "\n",
    "    return N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CO_FirstMin2(ts1, 'mi-hist', minNotMax=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Operations.CO_FirstCrossing import CO_FirstCrossing\n",
    "from Operations.CO_FirstMin import CO_FirstMin\n",
    "import numpy as np\n",
    "\n",
    "def CO_trev(y, tau = 'ac'):\n",
    "    \"\"\"\n",
    "    Normalized nonlinear autocorrelation, trev function of a time series.\n",
    "\n",
    "    Calculates the trev function, a normalized nonlinear autocorrelation,\n",
    "    mentioned in the documentation of the TSTOOL nonlinear time-series analysis\n",
    "    package.\n",
    "\n",
    "    Parameters:\n",
    "    y (array-like): Time series\n",
    "    tau (int, str, optional): Time lag. Can be 'ac' or 'mi' to set as the first \n",
    "                              zero-crossing of the autocorrelation function, or \n",
    "                              the first minimum of the automutual information \n",
    "                              function, respectively. Default is 'ac'.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the following keys:\n",
    "        - 'raw': The raw trev expression\n",
    "        - 'abs': The magnitude of the raw expression\n",
    "        - 'num': The numerator\n",
    "        - 'absnum': The magnitude of the numerator\n",
    "        - 'denom': The denominator\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If no valid setting for time delay is found.\n",
    "    \"\"\"\n",
    "\n",
    "    # Can set the time lag, tau, to be 'ac' or 'mi'\n",
    "    if tau == 'ac':\n",
    "        # tau is first zero crossing of the autocorrelation function\n",
    "        tau = CO_FirstCrossing(y, 'ac', 0, 'discrete')\n",
    "    elif tau == 'mi':\n",
    "        # tau is the first minimum of the automutual information function\n",
    "        tau = CO_FirstMin(y, 'mi')\n",
    "    if np.isnan(tau):\n",
    "        raise ValueError(\"No valid setting for time delay. (Is the time series too short?)\")\n",
    "\n",
    "    # Compute trev quantities\n",
    "    yn = y[:-tau]\n",
    "    yn1 = y[tau:] # yn, tau steps ahead\n",
    "    \n",
    "    out = {}\n",
    "\n",
    "    # The trev expression used in TSTOOL\n",
    "    raw = np.mean((yn1 - yn)**3) / (np.mean((yn1 - yn)**2))**(3/2)\n",
    "    out['raw'] = raw\n",
    "\n",
    "    # The magnitude\n",
    "    out['abs'] = np.abs(raw)\n",
    "\n",
    "    # The numerator\n",
    "    num = np.mean((yn1-yn)**3)\n",
    "    out['num'] = num\n",
    "    out['absnum'] = np.abs(num)\n",
    "\n",
    "    # the denominator\n",
    "    out['denom'] = (np.mean((yn1-yn)**2))**(3/2)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nitime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1 = np.loadtxt(\"ts1.txt\")\n",
    "ts2 = np.loadtxt(\"ts2.txt\")\n",
    "ts3 = np.loadtxt(\"ts3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Operations.CO_FirstCrossing import CO_FirstCrossing\n",
    "\n",
    "def CO_glscf(y, alpha, beta, tau = 'tau'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Set tau to first zero-crossing of the autocorrelation function with the input 'tau'\n",
    "    if tau == 'tau':\n",
    "        tau = CO_FirstCrossing(y, 'ac', 0, 'discrete')\n",
    "    \n",
    "    # Take magnitudes of time-delayed versions of the time series\n",
    "    y1 = np.abs(y[:-tau])\n",
    "    y2 = np.abs(y[tau:])\n",
    "\n",
    "\n",
    "    p1 = np.mean(np.multiply((y1 ** alpha), (y2 ** beta)))\n",
    "    p2 = np.multiply(np.mean(y1 ** alpha), np.mean(y2 ** beta))\n",
    "    p3 = np.sqrt(np.mean(y1 ** (2*alpha)) - (np.mean(y1 ** alpha))**2)\n",
    "    p4 = np.sqrt(np.mean(y2 ** (2*beta)) - (np.mean(y2 ** beta))**2)\n",
    "\n",
    "    glscf = (p1 - p2) / (p3 * p4)\n",
    "\n",
    "    return glscf    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25284356429315435"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CO_glscf(ts1, 0.9, 0.4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Operations.CO_FirstCrossing import CO_FirstCrossing\n",
    "from Operations.CO_glscf import CO_glscf\n",
    "\n",
    "def CO_fzcglscf(y, alpha, beta, maxtau = None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    N = len(y) # the length of the time series\n",
    "\n",
    "    if maxtau is None:\n",
    "        maxtau = N\n",
    "    \n",
    "    glscfs = np.zeros(maxtau)\n",
    "\n",
    "    for i in range(1, maxtau+1):\n",
    "        tau = i\n",
    "\n",
    "        glscfs[i-1] = CO_glscf(y, alpha, beta, tau)\n",
    "        if (i > 1) and (glscfs[i-1]*glscfs[i-2] < 0):\n",
    "            # Draw a straight line between these two and look at where it hits zero\n",
    "            out = i - 1 + glscfs[i-1]/(glscfs[i-1]-glscfs[i-2])\n",
    "            return out\n",
    "    \n",
    "    return maxtau # if the function hasn't exited yet, set output to maxtau \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "def DN_cv(x, k = 1):\n",
    "    \"\"\"\n",
    "    Coefficient of variation\n",
    "\n",
    "    Coefficient of variation of order k is sigma^k / mu^k (for sigma, standard\n",
    "    deviation and mu, mean) of a data vector, x\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    x (array-like): The input data vector\n",
    "    k (int, optional): The order of coefficient of variation (k = 1 is default)\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float: The coefficient of variation of order k\n",
    "    \"\"\"\n",
    "    if not isinstance(k, int) or k < 0:\n",
    "        warnings.warn('k should probably be a positive integer')\n",
    "        # Carry on with just this warning, though\n",
    "    \n",
    "    # Compute the coefficient of variation (of order k) of the data\n",
    "    return (np.std(x, ddof=1) ** k) / (np.mean(x) ** k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.269342687595913"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_cv(ts3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2765413621752804"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_nlogL_norm(ts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def DN_HighLowMu(y):\n",
    "    \"\"\"\n",
    "    The highlowmu statistic.\n",
    "\n",
    "    The highlowmu statistic is the ratio of the mean of the data that is above the\n",
    "    (global) mean compared to the mean of the data that is below the global mean.\n",
    "\n",
    "    Paramters:\n",
    "    ----------\n",
    "    y (array-like): The input data vector\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float: The highlowmu statistic.\n",
    "    \"\"\"\n",
    "    mu = np.mean(y) # mean of data\n",
    "    mhi = np.mean(y[y > mu]) # mean of data above the mean\n",
    "    mlo = np.mean(y[y < mu]) # mean of data below the mean\n",
    "    out = np.divide((mhi-mu), (mu-mlo)) # ratio of the differences\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0325203252032518"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_HighLowMu(ts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def BF_SimpleBinner(xData, numBins):\n",
    "    \"\"\"\n",
    "    Generate a histogram from equally spaced bins.\n",
    "   \n",
    "    Parameters:\n",
    "    xData (array-like): A data vector.\n",
    "    numBins (int): The number of bins.\n",
    "\n",
    "    Returns:\n",
    "    tuple: (N, binEdges)\n",
    "        N (numpy.ndarray): The counts\n",
    "        binEdges (numpy.ndarray): The extremities of the bins.\n",
    "    \"\"\"\n",
    "    minX = np.min(xData)\n",
    "    maxX = np.max(xData)\n",
    "    \n",
    "    # Linearly spaced bins:\n",
    "    binEdges = np.linspace(minX, maxX, numBins + 1)\n",
    "    N = np.zeros(numBins, dtype=int)\n",
    "    \n",
    "    for i in range(numBins):\n",
    "        if i < numBins - 1:\n",
    "            N[i] = np.sum((xData >= binEdges[i]) & (xData < binEdges[i+1]))\n",
    "        else:\n",
    "            # the final bin\n",
    "            N[i] = np.sum((xData >= binEdges[i]) & (xData <= binEdges[i+1]))\n",
    "    \n",
    "    return N, binEdges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def DN_Cumulants(y, cumWhatMay = 'skew1'):\n",
    "    \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    if cumWhatMay == 'skew1':\n",
    "        out = stats.skew(y)\n",
    "    elif cumWhatMay == 'skew2':\n",
    "        out = stats.skew(y, bias=False)\n",
    "    elif cumWhatMay == 'kurt1':\n",
    "        out = stats.kurtosis(y, fisher=False)\n",
    "    elif cumWhatMay == 'kurt2':\n",
    "        out = stats.kurtosis(y, bias=False, fisher=False)\n",
    "    else:\n",
    "        raise ValueError('Requested Unknown cumulant must be: skew1, skew2, kurt1, or kurt2')\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.497362974297833"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_Cumulants(ts1, cumWhatMay='kurt1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4958467355321061"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_Cumulants(ts1, cumWhatMay='kurt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def EN_ApEN(y, mnom = 1, rth = 0.2):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    r = rth * np.std(y, ddof=1) # threshold of similarity\n",
    "    N = len(y) # time series length\n",
    "    phi = np.zeros(2) # phi[0] = phi_m, phi[1] = phi_{m+1}\n",
    "\n",
    "    for k in range(2):\n",
    "        m = mnom+k # pattern length\n",
    "        C = np.zeros(N - m + 1)\n",
    "        # define the matrix x, containing subsequences of u\n",
    "        x = np.zeros((N-m+1, m))\n",
    "\n",
    "        # Form vector sequences x from the time series y\n",
    "        x = np.array([y[i:i+m] for i in range(N - m + 1)])\n",
    "        \n",
    "        for i in range(N - m + 1):\n",
    "            # Calculate the number of x[j] within r of x[i]\n",
    "            d = np.abs(x - x[i])\n",
    "            if m > 1:\n",
    "                d = np.max(d, axis=1)\n",
    "            C[i] = np.sum(d <= r) / (N - m + 1)\n",
    "\n",
    "        phi[k] = np.mean(np.log(C))\n",
    "\n",
    "    return phi[0] - phi[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.514020217676181"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EN_ApEN(ts3, 2, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.mlab\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import math\n",
    "import matplotlib\n",
    "\n",
    "def MD_hrv_classic(y):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Standard defaults\n",
    "    y = np.array(y)\n",
    "    diffy = np.diff(y)\n",
    "    N = len(y)\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Calculate pNNx percentage\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # pNNx: recommendation as per Mietus et. al. 2002, \"The pNNx files: ...\", Heart\n",
    "    # strange to do this for a z-scored time series...\n",
    "    Dy = np.abs(diffy)\n",
    "    PNNxfn = lambda x : np.mean(Dy > x/1000)\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    out['pnn5'] = PNNxfn(5) # 0.0055*sigma\n",
    "    out['pnn10'] = PNNxfn(10) # 0.01*sigma\n",
    "    out['pnn20'] = PNNxfn(20) # 0.02*sigma\n",
    "    out['pnn30'] = PNNxfn(30) # 0.03*sigma\n",
    "    out['pnn40'] = PNNxfn(40) # 0.04*sigma\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Calculate PSD\n",
    "    # ------------------------------------------------------------------------------\n",
    "\n",
    "    F, Pxx = signal.periodogram(y, window=np.hanning(N))\n",
    "\n",
    "    # Calculate spectral measures such as subband spectral power percentage, LF/HF ratio etc.\n",
    "    LF_lo = 0.04 # /pi -- fraction of total power (max F is pi)\n",
    "    LF_hi = 0.15\n",
    "    HF_lo = 0.15\n",
    "    HF_hi = 0.4\n",
    "\n",
    "    fbinsize = F[1] - F[0]\n",
    "\n",
    "    # Calculate PSD\n",
    "    f, Pxx = signal.periodogram(y, window='hann', detrend=False)\n",
    "    f *= 2 * np.pi\n",
    "    #print(Pxx)\n",
    "\n",
    "    # Calculate spectral measures\n",
    "    LF_lo, LF_hi = 0.04, 0.15\n",
    "    HF_lo, HF_hi = 0.15, 0.4\n",
    "\n",
    "    fbinsize = f[1] - f[0]\n",
    "    indl = (f >= LF_lo) & (f <= LF_hi)\n",
    "    indh = (f >= HF_lo) & (f <= HF_hi)\n",
    "    indv = f <= LF_lo\n",
    "\n",
    "    lfp = fbinsize * np.sum(Pxx[indl])\n",
    "    hfp = fbinsize * np.sum(Pxx[indh])\n",
    "    vlfp = fbinsize * np.sum(Pxx[indv])\n",
    "    total = fbinsize * np.sum(Pxx)\n",
    "\n",
    "    out['lfhf'] = lfp / hfp\n",
    "    out['vlf'] = vlfp / total * 100\n",
    "    out['lf'] = lfp / total * 100\n",
    "    out['hf'] = hfp / total * 100\n",
    "\n",
    "    # Triangular histogram index\n",
    "    numBins = 10\n",
    "    hist = np.histogram(y, bins=numBins)\n",
    "    out['tri'] = len(y)/np.max(hist[0])\n",
    "\n",
    "    # Poincare plot measures:\n",
    "    # cf. \"Do Existing Measures ... \", Brennan et. al. (2001), IEEE Trans Biomed Eng 48(11)\n",
    "    rmssd = np.std(diffy, ddof=1)\n",
    "    sigma = np.std(y, ddof=1)\n",
    "\n",
    "    out[\"SD1\"] = 1/math.sqrt(2) * rmssd * 1000\n",
    "    out[\"SD2\"] = math.sqrt(2 * sigma**2 - (1/2) * rmssd**2) * 1000\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pnn5': 0.982982982982983,\n",
       " 'pnn10': 0.9669669669669669,\n",
       " 'pnn20': 0.9359359359359359,\n",
       " 'pnn30': 0.9029029029029029,\n",
       " 'pnn40': 0.8708708708708709,\n",
       " 'lfhf': 8.4832685576491e-08,\n",
       " 'vlf': 1.1046723335508328e-08,\n",
       " 'lf': 8.483267835924858e-06,\n",
       " 'hf': 99.99999149238012,\n",
       " 'tri': 4.830917874396135,\n",
       " 'SD1': 99.72437694213063,\n",
       " 'SD2': 996.9513129028788}"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MD_hrv_classic(ts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def BF_MakeBuffer(y, bufferSize):\n",
    "    \"\"\"\n",
    "    Make a buffered version of a time series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array-like\n",
    "        The input time series.\n",
    "    buffer_size : int\n",
    "        The length of each buffer segment.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_buffer : ndarray\n",
    "        2D array where each row is a segment of length `buffer_size` \n",
    "        corresponding to consecutive, non-overlapping segments of the input time series.\n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "\n",
    "    numBuffers = int(np.floor(N/bufferSize))\n",
    "\n",
    "    # may need trimming\n",
    "    y_buffer = y[:numBuffers*bufferSize]\n",
    "    # then reshape\n",
    "    y_buffer = y_buffer.reshape((numBuffers,bufferSize))\n",
    "\n",
    "    return y_buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def EN_wentropy(y, whaten = 'shannon', p = None):\n",
    "    \"\"\"\n",
    "    Entropy of time series using wavelets.\n",
    "    Uses a python port of the MATLAB wavelet toolbox wentropy function.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    y : array_like\n",
    "        Input time series\n",
    "    whaten : str, optional\n",
    "        The entropy type:\n",
    "        - 'shannon' (default)\n",
    "        - 'logenergy'\n",
    "        - 'threshold' (with a given threshold)\n",
    "        - 'sure' (with a given parameter)\n",
    "        (see the wentropy documentation for information)\n",
    "    p : any, optional\n",
    "        the additional parameter needed for threshold and sure entropies\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    out : float\n",
    "        Entropy value. \n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "\n",
    "    if whaten == 'shannon':\n",
    "        # compute Shannon entropy\n",
    "        out = wentropy(y, 'shannon')/N\n",
    "    elif whaten == 'logenergy':\n",
    "        out = wentropy(y, 'logenergy')/N\n",
    "    elif whaten == 'threshold':\n",
    "        # check that p has been provided\n",
    "        if p is not None:\n",
    "            out = wentropy(y, 'threshold', p)/N\n",
    "        else:\n",
    "            raise ValueError(\"threshold requires an additional parameter, p.\")\n",
    "    elif whaten == 'sure':\n",
    "        if p is not None:\n",
    "            out = wentropy(y, 'sure', p)/N\n",
    "        else:\n",
    "            raise ValueError(\"sure requires an additional parameter, p.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown entropy type {whaten}\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# helper functions\n",
    "# taken from https://github.com/fairscape/hctsa-py/blob/master/PeripheryFunctions/wentropy.py\n",
    "def wentropy(x, entType = 'shannon', additionalParameter = None):\n",
    "\n",
    "    if entType == 'shannon':\n",
    "        x = np.power(x[ x != 0 ],2)\n",
    "        return - np.sum(np.multiply(x,np.log(x)))\n",
    "\n",
    "    elif entType == 'threshold':\n",
    "        if additionalParameter is None or isinstance(additionalParameter, str):\n",
    "            return None\n",
    "        x = np.absolute(x)\n",
    "        return np.sum((x > additionalParameter))\n",
    "\n",
    "    elif entType == 'norm':\n",
    "        if additionalParameter is None or isinstance(additionalParameter,str) or additionalParameter < 1:\n",
    "            return None\n",
    "        x = np.absolute(x)\n",
    "        return np.sum(np.power(x, additionalParameter))\n",
    "\n",
    "    elif entType == 'sure':\n",
    "        if additionalParameter is None or isinstance(additionalParameter,str):\n",
    "            return None\n",
    "\n",
    "        N = len(x)\n",
    "        x2 = np.square(x)\n",
    "        t2 = additionalParameter**2\n",
    "        xgt = np.sum((x2 > t2))\n",
    "        xlt = N - xgt\n",
    "\n",
    "        return N - (2*xlt) + (t2 *xgt) + np.sum(np.multiply(x2,(x2 <= t2)))\n",
    "\n",
    "    elif entType == 'logenergy':\n",
    "        x = np.square(x[x != 0])\n",
    "        return np.sum(np.log(x))\n",
    "\n",
    "    else:\n",
    "        print(\"invalid entropy type\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.22589264096854644"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EN_wentropy(ts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def DN_CustomSkewness(y, whatSkew = 'pearson'):\n",
    "    \"\"\"\n",
    "    Custom skewness measures\n",
    "    \"\"\"\n",
    "\n",
    "    if whatSkew == 'pearson':\n",
    "        out = ((3 * np.mean(y) - np.median(y)) / np.std(y, ddof=1))\n",
    "    elif whatSkew == 'bowley':\n",
    "        qs = np.quantile(y, [0.25, 0.5, 0.75], method='hazen')\n",
    "        out = (qs[2]+qs[0] - 2 * qs[1]) / (qs[2] - qs[0]) \n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.008252488684308562"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_CustomSkewness(ts3, 'bowley')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import norm, genextreme, uniform, beta, rayleigh, expon, gamma, lognorm, weibull_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x):\n",
    "    x = (x - np.min(x) + 0.01*np.std(x, ddof=1)) / (np.max(x) - np.min(x) + 0.01*np.std(x, ddof=1))\n",
    "    params = stats.beta.fit(x)\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5256618923359788, 0.5346381917755811, -0.0930931392643515, 1.0930931392643517)\n"
     ]
    }
   ],
   "source": [
    "test(ts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4431999999999996"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-2.2148 + 4.6579999999999995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SC_DFA(y):\n",
    "\n",
    "    N = len(y)\n",
    "\n",
    "    tau = int(np.floor(N/2))\n",
    "\n",
    "    y = y - np.mean(y)\n",
    "\n",
    "    x = np.cumsum(y)\n",
    "\n",
    "    taus = np.arange(5,tau+1)\n",
    "\n",
    "    ntau = len(taus)\n",
    "\n",
    "    F = np.zeros(ntau)\n",
    "\n",
    "    for i in range(ntau):\n",
    "\n",
    "        t = int(taus[i])\n",
    "\n",
    "\n",
    "\n",
    "        x_buff = x[:N - N % t]\n",
    "\n",
    "        x_buff = x_buff.reshape((int(N / t),t))\n",
    "\n",
    "\n",
    "        y_buff = np.zeros((int(N / t),t))\n",
    "\n",
    "        for j in range(int(N / t)):\n",
    "\n",
    "            tt = range(0,int(t))\n",
    "\n",
    "            p = np.polyfit(tt,x_buff[j,:],1)\n",
    "\n",
    "            y_buff[j,:] =  np.power(x_buff[j,:] - np.polyval(p,tt),2)\n",
    "\n",
    "\n",
    "\n",
    "        y_buff.reshape((N - N % t,1))\n",
    "\n",
    "        F[i] = np.sqrt(np.mean(y_buff))\n",
    "\n",
    "    logtaur = np.log(taus)\n",
    "\n",
    "    logF = np.log(F)\n",
    "\n",
    "    p = np.polyfit(logtaur,logF,1)\n",
    "\n",
    "    return p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2856214933647925"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SC_DFA(ts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math \n",
    "\n",
    "def CO_RM_AMInformation(y,tau = 1):\n",
    "    \"\"\"\n",
    "    A wrapper for rm_information(), which calculates automutal information\n",
    "\n",
    "    Inputs:\n",
    "        y, the input time series\n",
    "        tau, the time lag at which to calculate automutal information\n",
    "\n",
    "    :returns estimate of mutual information\n",
    "\n",
    "    - Wrapper initially developed by Ben D. Fulcher in MATLAB\n",
    "    - rm_information.py initially developed by Rudy Moddemeijer in MATLAB\n",
    "    - Translated to python by Tucker Cullen\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if tau >= len(y):\n",
    "\n",
    "        return\n",
    "\n",
    "    y1 = y[:-tau]\n",
    "    y2 = y[tau:]\n",
    "\n",
    "    out = RM_information(y1,y2)\n",
    "\n",
    "    return out[0]\n",
    "\n",
    "def RM_histogram2(*args):\n",
    "    \"\"\"\n",
    "    rm_histogram2() computes the two dimensional frequency histogram of two row vectors x and y\n",
    "\n",
    "    Takes in either two or three parameters:\n",
    "        rm_histogram(x, y)\n",
    "        rm_histogram(x, y, descriptor)\n",
    "\n",
    "    x, y : the row vectors to be analyzed\n",
    "    descriptor : the descriptor of the histogram where:\n",
    "\n",
    "        descriptor = [lowerx, upperx, ncellx, lowery, uppery, ncelly]\n",
    "            lower? : the lowerbound of the ? dimension of the histogram\n",
    "            upper? : the upperbound of the dimension of the histogram\n",
    "            ncell? : the number of cells of the ? dimension of the histogram\n",
    "\n",
    "    :return: a tuple countaining a) the result (the 2d frequency histogram), b) descriptor (the descriptor used)\n",
    "\n",
    "    MATLAB function and logic by Rudy Moddemeijer\n",
    "    Translated to python by Tucker Cullen\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    nargin = len(args)\n",
    "\n",
    "    if nargin < 1:\n",
    "        print(\"Usage: result = rm_histogram2(X, Y)\")\n",
    "        print(\"       result = rm_histogram2(X,Y)\")\n",
    "        print(\"Where: descriptor = [lowerX, upperX, ncellX; lowerY, upperY, ncellY\")\n",
    "\n",
    "    # some initial tests on the input arguments\n",
    "\n",
    "    x = np.array(args[0])  # make sure the imputs are in numpy array form\n",
    "    y = np.array(args[1])\n",
    "\n",
    "    xshape = x.shape\n",
    "    yshape = y.shape\n",
    "\n",
    "    lenx = xshape[0]  # how many elements are in the row vector\n",
    "    leny = yshape[0]\n",
    "\n",
    "    if len(xshape) != 1:  # makes sure x is a row vector\n",
    "        print(\"Error: invalid dimension of x\")\n",
    "        return\n",
    "\n",
    "    if len(yshape) != 1:\n",
    "        print(\"Error: invalid dimension of y\")\n",
    "        return\n",
    "\n",
    "    if lenx != leny:  # makes sure x and y have the same amount of elements\n",
    "        print(\"Error: unequal length of x and y\")\n",
    "        return\n",
    "\n",
    "    if nargin > 3:\n",
    "        print(\"Error: too many arguments\")\n",
    "        return\n",
    "\n",
    "    if nargin == 2:\n",
    "        minx = np.amin(x)\n",
    "        maxx = np.amax(x)\n",
    "        deltax = (maxx - minx) / (lenx - 1)\n",
    "        ncellx = math.ceil(lenx ** (1 / 3))\n",
    "\n",
    "        miny = np.amin(y)\n",
    "        maxy = np.amax(y)\n",
    "        deltay = (maxy - miny) / (leny - 1)\n",
    "        ncelly = ncellx\n",
    "        descriptor = np.array(\n",
    "            [[minx - deltax / 2, maxx + deltax / 2, ncellx], [miny - deltay / 2, maxy + deltay / 2, ncelly]])\n",
    "    else:\n",
    "        descriptor = args[2]\n",
    "\n",
    "    lowerx = descriptor[0, 0]  # python indexes one less then matlab indexes, since starts at zero\n",
    "    upperx = descriptor[0, 1]\n",
    "    ncellx = descriptor[0, 2]\n",
    "    lowery = descriptor[1, 0]\n",
    "    uppery = descriptor[1, 1]\n",
    "    ncelly = descriptor[1, 2]\n",
    "\n",
    "    # checking descriptor to make sure it is valid, otherwise print an error\n",
    "\n",
    "    if ncellx < 1:\n",
    "        print(\"Error: invalid number of cells in X dimension\")\n",
    "\n",
    "    if ncelly < 1:\n",
    "        print(\"Error: invalid number of cells in Y dimension\")\n",
    "\n",
    "    if upperx <= lowerx:\n",
    "        print(\"Error: invalid bounds in X dimension\")\n",
    "\n",
    "    if uppery <= lowery:\n",
    "        print(\"Error: invalid bounds in Y dimension\")\n",
    "\n",
    "    result = np.zeros([int(ncellx), int(ncelly)],\n",
    "                      dtype=int)  # should do the same thing as matlab: result(1:ncellx,1:ncelly) = 0;\n",
    "\n",
    "    xx = np.around((x - lowerx) / (upperx - lowerx) * ncellx + 1 / 2)\n",
    "    yy = np.around((y - lowery) / (uppery - lowery) * ncelly + 1 / 2)\n",
    "\n",
    "    xx = xx.astype(int)  # cast all the values in xx and yy to ints for use in indexing, already rounded in previous step\n",
    "    yy = yy.astype(int)\n",
    "\n",
    "    for n in range(0, lenx):\n",
    "        indexx = xx[n]\n",
    "        indexy = yy[n]\n",
    "\n",
    "        indexx -= 1  # adjust indices to start at zero, not one like in MATLAB\n",
    "        indexy -= 1\n",
    "\n",
    "        if indexx >= 0 and indexx <= ncellx - 1 and indexy >= 0 and indexy <= ncelly - 1:\n",
    "            result[indexx, indexy] = result[indexx, indexy] + 1\n",
    "\n",
    "    return result, descriptor\n",
    "\n",
    "def RM_information(*args):\n",
    "    \"\"\"\n",
    "    rm_information estimates the mutual information of the two stationary signals with\n",
    "    independent pairs of samples using various approaches:\n",
    "\n",
    "    takes in between 2 and 5 parameters:\n",
    "        rm_information(x, y)\n",
    "        rm_information(x, y, descriptor)\n",
    "        rm_information(x, y, descriptor, approach)\n",
    "        rm_information(x, y, descriptor, approach, base)\n",
    "\n",
    "    :returns estimate, nbias, sigma, descriptor\n",
    "\n",
    "        estimate : the mututal information estimate\n",
    "        nbias : n-bias of the estimate\n",
    "        sigma : the standard error of the estimate\n",
    "        descriptor : the descriptor of the histogram, see also rm_histogram2\n",
    "\n",
    "            lowerbound? : lowerbound of the histogram in the ? direction\n",
    "            upperbound? : upperbound of the histogram in the ? direction\n",
    "            ncell? : number of cells in the histogram in ? direction\n",
    "\n",
    "        approach : method used, choose from the following:\n",
    "\n",
    "            'unbiased'  : the unbiased estimate (default)\n",
    "            'mmse'      : minimum mean square estimate\n",
    "            'biased'    : the biased estimate\n",
    "\n",
    "        base : the base of the logarithm, default e\n",
    "\n",
    "    MATLAB function and logic by Rudy Moddemeijer\n",
    "    Translated to python by Tucker Cullen\n",
    "    \"\"\"\n",
    "\n",
    "    nargin = len(args)\n",
    "\n",
    "    if nargin < 1:\n",
    "        print(\"Takes in 2-5 parameters: \")\n",
    "        print(\"rm_information(x, y)\")\n",
    "        print(\"rm_information(x, y, descriptor)\")\n",
    "        print(\"rm_information(x, y, descriptor, approach)\")\n",
    "        print(\"rm_information(x, y, descriptor, approach, base)\")\n",
    "        print()\n",
    "\n",
    "        print(\"Returns a tuple containing: \")\n",
    "        print(\"estimate, nbias, sigma, descriptor\")\n",
    "        return\n",
    "\n",
    "    # some initial tests on the input arguments\n",
    "\n",
    "    x = np.array(args[0])  # make sure the imputs are in numpy array form\n",
    "    y = np.array(args[1])\n",
    "\n",
    "    xshape = x.shape\n",
    "    yshape = y.shape\n",
    "\n",
    "    lenx = xshape[0]  # how many elements are in the row vector\n",
    "    leny = yshape[0]\n",
    "\n",
    "    if len(xshape) != 1:  # makes sure x is a row vector\n",
    "        print(\"Error: invalid dimension of x\")\n",
    "        return\n",
    "\n",
    "    if len(yshape) != 1:\n",
    "        print(\"Error: invalid dimension of y\")\n",
    "        return\n",
    "\n",
    "    if lenx != leny:  # makes sure x and y have the same amount of elements\n",
    "        print(\"Error: unequal length of x and y\")\n",
    "        return\n",
    "\n",
    "    if nargin > 5:\n",
    "        print(\"Error: too many arguments\")\n",
    "        return\n",
    "\n",
    "    if nargin < 2:\n",
    "        print(\"Error: not enough arguments\")\n",
    "        return\n",
    "\n",
    "    # setting up variables depending on amount of inputs\n",
    "\n",
    "    if nargin == 2:\n",
    "        hist = RM_histogram2(x, y)  # call outside function from rm_histogram2.py\n",
    "        h = hist[0]\n",
    "        descriptor = hist[1]\n",
    "\n",
    "    if nargin >= 3:\n",
    "        hist = RM_histogram2(x, y, args[2])  # call outside function from rm_histogram2.py, args[2] represents the given descriptor\n",
    "        h = hist[0]\n",
    "        descriptor = hist[1]\n",
    "\n",
    "    if nargin < 4:\n",
    "        approach = 'unbiased'\n",
    "    else:\n",
    "        approach = args[3]\n",
    "\n",
    "    if nargin < 5:\n",
    "        base = math.e  # as in e = 2.71828\n",
    "    else:\n",
    "        base = args[4]\n",
    "\n",
    "    lowerboundx = descriptor[0, 0]  #not sure why most of these were included in the matlab script, most of them go unused\n",
    "    upperboundx = descriptor[0, 1]\n",
    "    ncellx = descriptor[0, 2]\n",
    "    lowerboundy = descriptor[1, 0]\n",
    "    upperboundy = descriptor[1, 1]\n",
    "    ncelly = descriptor[1, 2]\n",
    "\n",
    "    estimate = 0\n",
    "    sigma = 0\n",
    "    count = 0\n",
    "\n",
    "    # determine row and column sums\n",
    "\n",
    "    hy = np.sum(h, 0)\n",
    "    hx = np.sum(h, 1)\n",
    "\n",
    "    ncellx = ncellx.astype(int)\n",
    "    ncelly = ncelly.astype(int)\n",
    "\n",
    "    for nx in range(0, ncellx):\n",
    "        for ny in range(0, ncelly):\n",
    "            if h[nx, ny] != 0:\n",
    "                logf = math.log(h[nx, ny] / hx[nx] / hy[ny])\n",
    "            else:\n",
    "                logf = 0\n",
    "\n",
    "            count = count + h[nx, ny]\n",
    "            estimate = estimate + h[nx, ny] * logf\n",
    "            sigma = sigma + h[nx, ny] * (logf ** 2)\n",
    "\n",
    "    # biased estimate\n",
    "\n",
    "    estimate = estimate / count\n",
    "    sigma = math.sqrt((sigma / count - estimate ** 2) / (count - 1))\n",
    "    estimate = estimate + math.log(count)\n",
    "    nbias = (ncellx - 1) * (ncelly - 1) / (2 * count)\n",
    "\n",
    "    # conversion to unbiased estimate\n",
    "\n",
    "    if approach[0] == 'u':\n",
    "        estimate = estimate - nbias\n",
    "        nbias = 0\n",
    "\n",
    "        # conversion to minimum mse estimate\n",
    "\n",
    "    if approach[0] == 'm':\n",
    "        estimate = estimate - nbias\n",
    "        nbias = 0\n",
    "        lamda = (estimate ** 2) / ((estimate ** 2) + (sigma ** 2))\n",
    "        nbias = (1 - lamda) * estimate\n",
    "        estimate = lamda * estimate\n",
    "        sigma = lamda * sigma\n",
    "\n",
    "        # base transformations\n",
    "\n",
    "    estimate = estimate / math.log(base)\n",
    "    nbias = nbias / math.log(base)\n",
    "    sigma = sigma / math.log(base)\n",
    "\n",
    "    return estimate, nbias, sigma, descriptor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.010575122175197689"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CO_RM_AMInformation(ts3, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

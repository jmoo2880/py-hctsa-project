{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1 = np.loadtxt(\"ts1.txt\")\n",
    "ts2 = np.loadtxt(\"ts2.txt\")\n",
    "ts3 = np.loadtxt(\"ts3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Operations.CO_FirstCrossing import CO_FirstCrossing\n",
    "\n",
    "def CO_glscf(y, alpha, beta, tau = 'tau'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Set tau to first zero-crossing of the autocorrelation function with the input 'tau'\n",
    "    if tau == 'tau':\n",
    "        tau = CO_FirstCrossing(y, 'ac', 0, 'discrete')\n",
    "    \n",
    "    # Take magnitudes of time-delayed versions of the time series\n",
    "    y1 = np.abs(y[:-tau])\n",
    "    y2 = np.abs(y[tau:])\n",
    "\n",
    "\n",
    "    p1 = np.mean(np.multiply((y1 ** alpha), (y2 ** beta)))\n",
    "    p2 = np.multiply(np.mean(y1 ** alpha), np.mean(y2 ** beta))\n",
    "    p3 = np.sqrt(np.mean(y1 ** (2*alpha)) - (np.mean(y1 ** alpha))**2)\n",
    "    p4 = np.sqrt(np.mean(y2 ** (2*beta)) - (np.mean(y2 ** beta))**2)\n",
    "\n",
    "    glscf = (p1 - p2) / (p3 * p4)\n",
    "\n",
    "    return glscf    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25284356429315435"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CO_glscf(ts1, 0.9, 0.4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Operations.CO_FirstCrossing import CO_FirstCrossing\n",
    "from Operations.CO_glscf import CO_glscf\n",
    "\n",
    "def CO_fzcglscf(y, alpha, beta, maxtau = None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    N = len(y) # the length of the time series\n",
    "\n",
    "    if maxtau is None:\n",
    "        maxtau = N\n",
    "    \n",
    "    glscfs = np.zeros(maxtau)\n",
    "\n",
    "    for i in range(1, maxtau+1):\n",
    "        tau = i\n",
    "\n",
    "        glscfs[i-1] = CO_glscf(y, alpha, beta, tau)\n",
    "        if (i > 1) and (glscfs[i-1]*glscfs[i-2] < 0):\n",
    "            # Draw a straight line between these two and look at where it hits zero\n",
    "            out = i - 1 + glscfs[i-1]/(glscfs[i-1]-glscfs[i-2])\n",
    "            return out\n",
    "    \n",
    "    return maxtau # if the function hasn't exited yet, set output to maxtau \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "def DN_cv(x, k = 1):\n",
    "    \"\"\"\n",
    "    Coefficient of variation\n",
    "\n",
    "    Coefficient of variation of order k is sigma^k / mu^k (for sigma, standard\n",
    "    deviation and mu, mean) of a data vector, x\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    x (array-like): The input data vector\n",
    "    k (int, optional): The order of coefficient of variation (k = 1 is default)\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float: The coefficient of variation of order k\n",
    "    \"\"\"\n",
    "    if not isinstance(k, int) or k < 0:\n",
    "        warnings.warn('k should probably be a positive integer')\n",
    "        # Carry on with just this warning, though\n",
    "    \n",
    "    # Compute the coefficient of variation (of order k) of the data\n",
    "    return (np.std(x, ddof=1) ** k) / (np.mean(x) ** k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.269342687595913"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_cv(ts3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2765413621752804"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_nlogL_norm(ts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def DN_HighLowMu(y):\n",
    "    \"\"\"\n",
    "    The highlowmu statistic.\n",
    "\n",
    "    The highlowmu statistic is the ratio of the mean of the data that is above the\n",
    "    (global) mean compared to the mean of the data that is below the global mean.\n",
    "\n",
    "    Paramters:\n",
    "    ----------\n",
    "    y (array-like): The input data vector\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float: The highlowmu statistic.\n",
    "    \"\"\"\n",
    "    mu = np.mean(y) # mean of data\n",
    "    mhi = np.mean(y[y > mu]) # mean of data above the mean\n",
    "    mlo = np.mean(y[y < mu]) # mean of data below the mean\n",
    "    out = np.divide((mhi-mu), (mu-mlo)) # ratio of the differences\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0325203252032518"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_HighLowMu(ts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def BF_SimpleBinner(xData, numBins):\n",
    "    \"\"\"\n",
    "    Generate a histogram from equally spaced bins.\n",
    "   \n",
    "    Parameters:\n",
    "    xData (array-like): A data vector.\n",
    "    numBins (int): The number of bins.\n",
    "\n",
    "    Returns:\n",
    "    tuple: (N, binEdges)\n",
    "        N (numpy.ndarray): The counts\n",
    "        binEdges (numpy.ndarray): The extremities of the bins.\n",
    "    \"\"\"\n",
    "    minX = np.min(xData)\n",
    "    maxX = np.max(xData)\n",
    "    \n",
    "    # Linearly spaced bins:\n",
    "    binEdges = np.linspace(minX, maxX, numBins + 1)\n",
    "    N = np.zeros(numBins, dtype=int)\n",
    "    \n",
    "    for i in range(numBins):\n",
    "        if i < numBins - 1:\n",
    "            N[i] = np.sum((xData >= binEdges[i]) & (xData < binEdges[i+1]))\n",
    "        else:\n",
    "            # the final bin\n",
    "            N[i] = np.sum((xData >= binEdges[i]) & (xData <= binEdges[i+1]))\n",
    "    \n",
    "    return N, binEdges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def DN_Cumulants(y, cumWhatMay = 'skew1'):\n",
    "    \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    if cumWhatMay == 'skew1':\n",
    "        out = stats.skew(y)\n",
    "    elif cumWhatMay == 'skew2':\n",
    "        out = stats.skew(y, bias=False)\n",
    "    elif cumWhatMay == 'kurt1':\n",
    "        out = stats.kurtosis(y, fisher=False)\n",
    "    elif cumWhatMay == 'kurt2':\n",
    "        out = stats.kurtosis(y, bias=False, fisher=False)\n",
    "    else:\n",
    "        raise ValueError('Requested Unknown cumulant must be: skew1, skew2, kurt1, or kurt2')\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.497362974297833"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_Cumulants(ts1, cumWhatMay='kurt1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4958467355321061"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_Cumulants(ts1, cumWhatMay='kurt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def EN_ApEN(y, mnom = 1, rth = 0.2):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    r = rth * np.std(y, ddof=1) # threshold of similarity\n",
    "    N = len(y) # time series length\n",
    "    phi = np.zeros(2) # phi[0] = phi_m, phi[1] = phi_{m+1}\n",
    "\n",
    "    for k in range(2):\n",
    "        m = mnom+k # pattern length\n",
    "        C = np.zeros(N - m + 1)\n",
    "        # define the matrix x, containing subsequences of u\n",
    "        x = np.zeros((N-m+1, m))\n",
    "\n",
    "        # Form vector sequences x from the time series y\n",
    "        x = np.array([y[i:i+m] for i in range(N - m + 1)])\n",
    "        \n",
    "        for i in range(N - m + 1):\n",
    "            # Calculate the number of x[j] within r of x[i]\n",
    "            d = np.abs(x - x[i])\n",
    "            if m > 1:\n",
    "                d = np.max(d, axis=1)\n",
    "            C[i] = np.sum(d <= r) / (N - m + 1)\n",
    "\n",
    "        phi[k] = np.mean(np.log(C))\n",
    "\n",
    "    return phi[0] - phi[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.514020217676181"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EN_ApEN(ts3, 2, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Operations.SB_CoarseGrain import SB_CoarseGrain\n",
    "\n",
    "def SB_MotifThree(y, cgHow = 'quantile'):\n",
    "    \"\"\"\n",
    "    Motifs in a coarse-graining of a time series to a 3-letter alphabet.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y : np.ndarray\n",
    "        Time series to analyze.\n",
    "    cg_how : {'quantile', 'diffquant'}, optional\n",
    "        The coarse-graining method to use:\n",
    "        - 'quantile': equiprobable alphabet by time-series value\n",
    "        - 'diffquant': equiprobably alphabet by time-series increments\n",
    "        Default is 'quantile'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Dict[str, float]\n",
    "        Statistics on words of length 1, 2, 3, and 4.\n",
    "    \"\"\"\n",
    "\n",
    "    # Coarse-grain the data y -> yt\n",
    "    numLetters = 3\n",
    "    if cgHow == 'quantile':\n",
    "        yt = SB_CoarseGrain(y, 'quantile', numLetters)\n",
    "    elif cgHow == 'diffquant':\n",
    "        yt = SB_CoarseGrain(np.diff(y), 'quantile', numLetters)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown coarse-graining method {cgHow}\")\n",
    "\n",
    "    # So we have a vectory yt with entries in {1, 2, 3}\n",
    "    N = len(yt) # length of the symbolized sequence derived from the time series\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Words of length 1\n",
    "    # ------------------------------------------------------------------------------\n",
    "    out1 = np.zeros(3)\n",
    "    r1 = [np.where(yt == i + 1)[0] for i in range(3)]\n",
    "    for i in range(3):\n",
    "        out1[i] = len(r1[i]) / N\n",
    "\n",
    "    out = {\n",
    "        'a': out1[0], 'b': out1[1], 'c': out1[2],\n",
    "        'h': f_entropy(out1)\n",
    "    }\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Words of length 2\n",
    "    # ------------------------------------------------------------------------------\n",
    "\n",
    "    r1 = [r[:-1] if len(r) > 0 and r[-1] == N - 1 else r for r in r1]\n",
    "    out2 = np.zeros((3, 3))\n",
    "    r2 = [[r1[i][yt[r1[i] + 1] == j + 1] for j in range(3)] for i in range(3)]\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            out2[i, j] = len(r2[i][j]) / (N - 1)\n",
    "\n",
    "    out.update({\n",
    "        'aa': out2[0, 0], 'ab': out2[0, 1], 'ac': out2[0, 2],\n",
    "        'ba': out2[1, 0], 'bb': out2[1, 1], 'bc': out2[1, 2],\n",
    "        'ca': out2[2, 0], 'cb': out2[2, 1], 'cc': out2[2, 2],\n",
    "        'hh': f_entropy(out2)\n",
    "    })\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Words of length 3\n",
    "    # ------------------------------------------------------------------------------\n",
    "\n",
    "    r2 = [[r[:-1] if len(r) > 0 and r[-1] == N - 2 else r for r in row] for row in r2]\n",
    "    out3 = np.zeros((3, 3, 3))\n",
    "    r3 = [[[r2[i][j][yt[r2[i][j] + 2] == k + 1] for k in range(3)] for j in range(3)] for i in range(3)]\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            for k in range(3):\n",
    "                out3[i, j, k] = len(r3[i][j][k]) / (N - 2)\n",
    "\n",
    "    out.update({f'{chr(97+i)}{chr(97+j)}{chr(97+k)}': out3[i, j, k] \n",
    "                for i in range(3) for j in range(3) for k in range(3)})\n",
    "    out['hhh'] = f_entropy(out3)\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Words of length 4\n",
    "    # ------------------------------------------------------------------------------\n",
    "\n",
    "    r3 = [[[r[:-1] if len(r) > 0 and r[-1] == N - 3 else r for r in plane] for plane in cube] for cube in r3]\n",
    "    out4 = np.zeros((3, 3, 3, 3))\n",
    "    r4 = [[[[r3[i][j][k][yt[r3[i][j][k] + 3] == l + 1] for l in range(3)] for k in range(3)] for j in range(3)] for i in range(3)]\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            for k in range(3):\n",
    "                for l in range(3):\n",
    "                    out4[i, j, k, l] = len(r4[i][j][k][l]) / (N - 3)\n",
    "\n",
    "    out.update({f'{chr(97+i)}{chr(97+j)}{chr(97+k)}{chr(97+l)}': out4[i, j, k, l] \n",
    "                for i in range(3) for j in range(3) for k in range(3) for l in range(3)})\n",
    "    out['hhhh'] = f_entropy(out4)\n",
    "\n",
    "    return out\n",
    "\n",
    "# helper function \n",
    "def f_entropy(x):\n",
    "    \"\"\"Entropy of a set of counts, log(0) = 0\"\"\"\n",
    "    return -np.sum(x[x > 0] * np.log(x[x > 0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1 = np.loadtxt(\"ts1.txt\")\n",
    "ts2 = np.loadtxt(\"ts2.txt\")\n",
    "ts3 = np.loadtxt(\"ts3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Operations.CO_FirstCrossing import CO_FirstCrossing\n",
    "\n",
    "def CO_HistogramAMI(y, tau = 1, meth = 'even', numBins = 10):\n",
    "    \"\"\"\n",
    "    CO_HistogramAMI: The automutual information of the distribution using histograms.\n",
    "\n",
    "    Parameters:\n",
    "    y (array-like): The input time series\n",
    "    tau (int, list or str): The time-lag(s) (default: 1)\n",
    "    meth (str): The method of computing automutual information:\n",
    "                'even': evenly-spaced bins through the range of the time series,\n",
    "                'std1', 'std2': bins that extend only up to a multiple of the\n",
    "                                standard deviation from the mean of the time series to exclude outliers,\n",
    "                'quantiles': equiprobable bins chosen using quantiles.\n",
    "    num_bins (int): The number of bins (default: 10)\n",
    "\n",
    "    Returns:\n",
    "    float or dict: The automutual information calculated in this way.\n",
    "    \"\"\"\n",
    "    # Use first zero crossing of the ACF as the time lag\n",
    "    if isinstance(tau, str) and tau in ['ac', 'tau']:\n",
    "        tau = CO_FirstCrossing(y, 'ac', 0, 'discrete')\n",
    "    \n",
    "    # Bins for the data\n",
    "    # same for both -- assume same distribution (true for stationary processes, or small lags)\n",
    "    if meth == 'even':\n",
    "        b = np.linspace(np.min(y), np.max(y), numBins + 1)\n",
    "        # Add increment buffer to ensure all points are included\n",
    "        inc = 0.1\n",
    "        b[0] -= inc\n",
    "        b[-1] += inc\n",
    "    elif meth == 'std1': # bins out to +/- 1 std\n",
    "        b = np.linspace(-1, 1, numBins + 1)\n",
    "        if np.min(y) < -1:\n",
    "            b = np.concatenate(([np.min(y) - 0.1], b))\n",
    "        if np.max(y) > 1:\n",
    "            b = np.concatenate((b, [np.max(y) + 0.1]))\n",
    "    elif meth == 'std2': # bins out to +/- 2 std\n",
    "        b = np.linspace(-2, 2, numBins + 1)\n",
    "        if np.min(y) < -2:\n",
    "            b = np.concatenate(([np.min(y) - 0.1], b))\n",
    "        if np.max(y) > 2:\n",
    "            b = np.concatenate((b, [np.max(y) + 0.1]))\n",
    "    elif meth == 'quantiles': # use quantiles with ~equal number in each bin\n",
    "        b = np.quantile(y, np.linspace(0, 1, numBins + 1))\n",
    "        b[0] -= 0.1\n",
    "        b[-1] += 0.1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method '{meth}'\")\n",
    "    \n",
    "    # Sometimes bins can be added (e.g., with std1 and std2), so need to redefine numBins\n",
    "    numBins = len(b) - 1\n",
    "\n",
    "    # Form the time-delay vectors y1 and y2\n",
    "    if not isinstance(tau, (list, np.ndarray)):\n",
    "        # if only single time delay as integer, make into a one element list\n",
    "        tau = [tau]\n",
    "\n",
    "    amis = np.zeros(len(tau))\n",
    "\n",
    "    for i, t in enumerate(tau):\n",
    "        y1 = y[:-t]\n",
    "        y2 = y[t:]\n",
    "\n",
    "        # Joint distribution of y1 and y2\n",
    "        pij, _, _ = np.histogram2d(y1, y2, bins=(b, b))\n",
    "        pij = pij[:numBins, :numBins]  # joint\n",
    "        pij = pij / np.sum(pij)  # normalize\n",
    "        pi = np.sum(pij, axis=1)  # marginal\n",
    "        pj = np.sum(pij, axis=0)  # other marginal\n",
    "\n",
    "        pii = np.tile(pi, (numBins, 1)).T\n",
    "        pjj = np.tile(pj, (numBins, 1))\n",
    "\n",
    "        r = pij > 0  # Defining the range in this way, we set log(0) = 0\n",
    "        amis[i] = np.sum(pij[r] * np.log(pij[r] / pii[r] / pjj[r]))\n",
    "\n",
    "    if len(tau) == 1:\n",
    "        return amis[0]\n",
    "    else:\n",
    "        return {f'ami{i+1}': ami for i, ami in enumerate(amis)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04761744653583484"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CO_HistogramAMI(ts3, 'ac', meth='quantiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0797345972733234"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.kurtosis(ts3_zs, fisher=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepBinary(X):\n",
    "    # Transform real values to 0 if <=0 and 1 if >0:\n",
    "    Y = np.zeros(len(X))\n",
    "    Y[X > 0] = 1\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "expFunc = lambda x, a, b : a * np.exp(b * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def firstUnder_fn(x, m, p):\n",
    "    \"\"\"\n",
    "    Find the value of m for the first time p goes under the threshold, x. \n",
    "    p and m vectors of the same length\n",
    "    \"\"\"\n",
    "    first_i = next((m_val for m_val, p_val in zip(m, p) if p_val < x), m[-1])\n",
    "    return first_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BF_Binarize(y, binarizeHow='diff'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if binarizeHow == 'diff':\n",
    "        # Binary signal: 1 for stepwise increases, 0 for stepwise decreases\n",
    "        yBin = stepBinary(np.diff(y))\n",
    "    \n",
    "    elif binarizeHow == 'mean':\n",
    "        # Binary signal: 1 for above mean, 0 for below mean\n",
    "        yBin = stepBinary(y - np.mean(y))\n",
    "    \n",
    "    elif binarizeHow == 'median':\n",
    "        # Binary signal: 1 for above median, 0 for below median\n",
    "        yBin = stepBinary(y - np.median(y))\n",
    "    \n",
    "    elif binarizeHow == 'iqr':\n",
    "        # Binary signal: 1 if inside interquartile range, 0 otherwise\n",
    "        iqr = np.quantile(y,[.25,.75])\n",
    "        iniqr = np.logical_and(y > iqr[0], y<iqr[1])\n",
    "        yBin = np.zeros(len(y))\n",
    "        yBin[iniqr] = 1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown binary transformation setting '{binarizeHow}'\")\n",
    "\n",
    "    return yBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mea = BF_Binarize(ts1, binarizeHow='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.random.randn(3, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.size(mat, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BF_SignChange(y, doFind=0):\n",
    "    \"\"\"\n",
    "    Where a data vector changes sign.\n",
    "\n",
    "    \"\"\"\n",
    "    if doFind == 0:\n",
    "        return (np.multiply(y[1:],y[0:len(y)-1]) < 0)\n",
    "    indexs = np.where((np.multiply(y[1:],y[0:len(y)-1]) < 0))\n",
    "\n",
    "    return indexs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = BF_SignChange(ts1, doFind=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 14,  30,  46,  61,  77,  93, 108, 124, 140, 156, 171, 187, 203,\n",
       "        218, 234, 250, 266, 281, 297, 313, 328, 344, 360, 375, 391, 407,\n",
       "        423, 438, 454, 470, 485, 501, 517, 533, 548, 564, 580, 595, 611,\n",
       "        627, 643, 658, 674, 690, 705, 721, 737, 752, 768, 784, 800, 815,\n",
       "        831, 847, 862, 878, 894, 910, 925, 941, 957, 972, 988]),)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import moment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DN_Moments(y, theMom):\n",
    "    \"\"\"\n",
    "    A moment of the distribution of the input time series.\n",
    "    \n",
    "    \"\"\"\n",
    "    out = moment(y, theMom) / np.std(y) # normalized\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.005128976116081682"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_Moments(ts1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def DN_Mean(y, mean_type='arithmetic'):\n",
    "    \"\"\"\n",
    "    A given measure of location of a data vector.\n",
    "\n",
    "    Parameters:\n",
    "    y (array-like): The input data vector\n",
    "    mean_type (str): The type of mean to calculate\n",
    "        'norm' or 'arithmetic': arithmetic mean\n",
    "        'median': median\n",
    "        'geom': geometric mean\n",
    "        'harm': harmonic mean\n",
    "        'rms': root-mean-square\n",
    "        'iqm': interquartile mean\n",
    "        'midhinge': midhinge\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated mean value\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If an unknown mean type is specified\n",
    "    \"\"\"\n",
    "    y = np.array(y)\n",
    "    N = len(y)\n",
    "\n",
    "    if mean_type in ['norm', 'arithmetic']:\n",
    "        return np.mean(y)\n",
    "    elif mean_type == 'median':\n",
    "        return np.median(y)\n",
    "    elif mean_type == 'geom':\n",
    "        return stats.gmean(y)\n",
    "    elif mean_type == 'harm':\n",
    "        return stats.hmean(y)\n",
    "    elif mean_type == 'rms':\n",
    "        return np.sqrt(np.mean(y**2))\n",
    "    elif mean_type == 'iqm':\n",
    "        p = np.percentile(y, [25, 75])\n",
    "        return np.mean(y[(y >= p[0]) & (y <= p[1])])\n",
    "    elif mean_type == 'midhinge':\n",
    "        p = np.percentile(y, [25, 75])\n",
    "        return np.mean(p)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mean type '{mean_type}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DN_ProportionValues(x, propWhat='positive'):\n",
    "\n",
    "    N = len(x)\n",
    "\n",
    "    if propWhat == 'zeros':\n",
    "        # returns the proportion of zeros in the input vector\n",
    "        out = sum(x == 0) / N\n",
    "    elif propWhat == 'positive':\n",
    "        out = sum(x > 0) / N\n",
    "    elif propWhat == 'geq0':\n",
    "        out = sum(x >= 0) / N\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown condition to measure: {propWhat}\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.519"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_ProportionValues(ts3, 'geq0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DN_Quantile(y, p=0.5):\n",
    "    \"\"\"\n",
    "    Calculates the quantile value at a specified proportion, p.\n",
    "\n",
    "    Parameters:\n",
    "    y (array-like): The input data vector\n",
    "    p (float): The quantile proportion (default is 0.5, which is the median)\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated quantile value\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If p is not a number between 0 and 1\n",
    "    \"\"\"\n",
    "    if p == 0.5:\n",
    "        print(\"Using quantile p = 0.5 (median) by default\")\n",
    "    \n",
    "    if not isinstance(p, (int, float)) or p < 0 or p > 1:\n",
    "        raise ValueError(\"p must specify a proportion, in (0,1)\")\n",
    "    \n",
    "    return np.quantile(y, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.58822"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_Quantile(ts1, p=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform, norm, geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'geom_gen' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgeom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m(ts2)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'geom_gen' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "geom.fit(ts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EN_CID(y):\n",
    "    \"\"\"\n",
    "    Simple complexity measure of a time series.\n",
    "\n",
    "    Estimates of 'complexity' of a time series as the stretched-out length of the\n",
    "    lines resulting from a line-graph of the time series.\n",
    "\n",
    "    Parameters:\n",
    "    y (array-like): the input time series\n",
    "\n",
    "    Returns:\n",
    "    out (dict): \n",
    "    \"\"\"\n",
    "    CE1 = f_CE1(y)\n",
    "    CE2 = f_CE2(y)\n",
    "\n",
    "    minCE1 = f_CE1(np.sort(y))\n",
    "    minCE2 = f_CE2(np.sort(y))\n",
    "\n",
    "    CE1_norm = CE1 / minCE1\n",
    "    CE2_norm = CE2 / minCE2\n",
    "\n",
    "    out = {'CE1':CE1,'CE2':CE2,'minCE1':minCE1,'minCE2':minCE2,\n",
    "            'CE1_norm':CE1_norm,'CE2_norm':CE2_norm}\n",
    "\n",
    "    return out\n",
    "\n",
    "def f_CE1(y):\n",
    "    return np.sqrt(np.mean(np.power(np.diff(y),2)))\n",
    "\n",
    "def f_CE2(y):\n",
    "    return np.mean(np.sqrt(1 + np.power(np.diff(y),2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CE1': 1.4397258123179755,\n",
       " 'CE2': 1.623039534424403,\n",
       " 'minCE1': 0.028789278256714637,\n",
       " 'minCE2': 1.0003926815512982,\n",
       " 'CE1_norm': 50.00909711872275,\n",
       " 'CE2_norm': 1.6224024469147185}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EN_CID(ts3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DN_Spread(y, spreadMeasure='std'):\n",
    "    \"\"\"\n",
    "    Measure of spread of the input time series.\n",
    "    Returns the spread of the raw data vector, as the standard deviation,\n",
    "    inter-quartile range, mean absolute deviation, or median absolute deviation.\n",
    "    \"\"\"\n",
    "    if spreadMeasure == 'std':\n",
    "        out = np.std(y)\n",
    "    elif spreadMeasure == 'iqr':\n",
    "        out = stats.iqr(y)\n",
    "    elif spreadMeasure == 'mad':\n",
    "        out = mad(y)\n",
    "    elif spreadMeasure == 'mead':\n",
    "        out = mead(y)\n",
    "    else:\n",
    "        raise ValueError('spreadMeasure must be one of std, iqr, mad or mead')\n",
    "\n",
    "    return out\n",
    "\n",
    "def mad(data, axis=None):\n",
    "    return np.mean(np.absolute(data - np.mean(data, axis)), axis)\n",
    "\n",
    "def mead(data, axis=None):\n",
    "    return np.median(np.absolute(data - np.median(data, axis)), axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66015"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_Spread(ts2, spreadMeasure='mead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DN_Unique(x):\n",
    "    \"\"\"\n",
    "    The proportion of the time series that are unique values.\n",
    "\n",
    "    Parameters:\n",
    "    x (array-like): the input data vector\n",
    "\n",
    "    Returns:\n",
    "    out (float): the proportion of time series that are unique values\n",
    "    \"\"\"\n",
    "\n",
    "    return len(np.unique(x)) / len(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.977"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_Unique(ts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "897"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(ts1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CO_NonlinearAutocorr(y,taus,doAbs ='empty'):\n",
    "\n",
    "    if doAbs == 'empty':\n",
    "\n",
    "        if len(taus) % 2 == 1:\n",
    "\n",
    "            doAbs = 0\n",
    "\n",
    "        else:\n",
    "\n",
    "            doAbs = 1\n",
    "\n",
    "    N = len(y)\n",
    "    tmax = np.max(taus)\n",
    "\n",
    "    nlac = y[tmax:N]\n",
    "\n",
    "    for i in taus:\n",
    "\n",
    "        nlac = np.multiply(nlac,y[ tmax - i:N - i ])\n",
    "\n",
    "    if doAbs:\n",
    "\n",
    "        return np.mean(np.absolute(nlac))\n",
    "\n",
    "    else:\n",
    "\n",
    "        return np.mean(nlac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32868826608330426"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CO_NonlinearAutocorr(ts1, [1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import trim_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DN_TrimmedMean(y, n=0):\n",
    "    \"\"\"\n",
    "    Mean of the trimmed time series using trimmean.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    y (array-like): the input time series\n",
    "    n (float): the fraction of highest and lowest values in y to exclude from the mean calculation\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    out (float): the mean of the trimmed time series.\n",
    "    \"\"\"\n",
    "    n *= 0.01\n",
    "    N = len(y)\n",
    "    trim = int(np.round(N * n / 2))\n",
    "    y = np.sort(y)\n",
    "\n",
    "    out = np.mean(y[trim:N-trim])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0023594444444444205"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_TrimmedMean(ts1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DN_Burstiness(y):\n",
    "    \"\"\"\n",
    "    Calculate the burstiness statistic of a time series.\n",
    "\n",
    "    This function returns the 'burstiness' statistic as defined in\n",
    "    Goh and Barabasi's paper, \"Burstiness and memory in complex systems,\"\n",
    "    Europhys. Lett. 81, 48002 (2008).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array-like\n",
    "        The input time series.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The original burstiness statistic, B, and the improved\n",
    "        burstiness statistic, B_Kim.\n",
    "    \"\"\"\n",
    "    \n",
    "    mean = np.mean(y)\n",
    "    std = np.std(y)\n",
    "\n",
    "    r = np.divide(std,mean) # coefficient of variation\n",
    "    B = np.divide((r - 1), (r + 1)) # Original Goh and Barabasi burstiness statistic, B\n",
    "\n",
    "    # improved burstiness statistic, accounting for scaling for finite time series\n",
    "    # Kim and Jo, 2016, http://arxiv.org/pdf/1604.01125v1.pdf\n",
    "    N = len(y)\n",
    "    p1 = np.sqrt(N+1)*r - np.sqrt(N-1)\n",
    "    p2 = (np.sqrt(N+1)-2)*r + np.sqrt(N-1)\n",
    "\n",
    "    B_Kim = np.divide(p1, p2)\n",
    "\n",
    "    out = {'B': B, 'B_Kim': B_Kim}\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B': 0.92662389689055, 'B_Kim': 0.9867869006600554}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_Burstiness(ts3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Operations.CO_HistogramAMI import CO_HistogramAMI\n",
    "from Operations.CO_FirstCrossing import CO_FirstCrossing\n",
    "from Operations.IN_AutoMutualInfo import IN_AutoMutualInfo\n",
    "from Operations.CO_AutoCorr import CO_AutoCorr\n",
    "from PeripheryFunctions.BF_SignChange import BF_SignChange\n",
    "from PeripheryFunctions.BF_iszscored import BF_iszscored\n",
    "from scipy.optimize import curve_fit\n",
    "import warnings\n",
    "\n",
    "def CO_AddNoise(y, tau = 1, amiMethod = 'even', extraParam = None, randomSeed = None):\n",
    "    \"\"\"\n",
    "    CO_AddNoise: Changes in the automutual information with the addition of noise\n",
    "\n",
    "    Parameters:\n",
    "    y (array-like): The input time series (should be z-scored)\n",
    "    tau (int or str): The time delay for computing AMI (default: 1)\n",
    "    amiMethod (str): The method for computing AMI:\n",
    "                      'std1','std2','quantiles','even' for histogram-based estimation,\n",
    "                      'gaussian','kernel','kraskov1','kraskov2' for estimation using JIDT\n",
    "    extraParam: e.g., the number of bins input to CO_HistogramAMI, or parameter for IN_AutoMutualInfo\n",
    "    randomSeed (int): Settings for resetting the random seed for reproducible results\n",
    "\n",
    "    Returns:\n",
    "    dict: Statistics on the resulting set of automutual information estimates\n",
    "    \"\"\"\n",
    "\n",
    "    if not BF_iszscored(y):\n",
    "        warnings.warn(\"Input time series should be z-scored\")\n",
    "    \n",
    "    # Set tau to minimum of autocorrelation function if 'ac' or 'tau'\n",
    "    if tau in ['ac', 'tau']:\n",
    "        tau = CO_FirstCrossing(y, 'ac', 0, 'discrete')\n",
    "    \n",
    "    # Generate noise\n",
    "    if randomSeed is not None:\n",
    "        np.random.seed(randomSeed)\n",
    "    noise = np.random.randn(len(y)) # generate uncorrelated additive noise\n",
    "\n",
    "    # Set up noise range\n",
    "    noiseRange = np.linspace(0, 3, 50) # compare properties across this noise range\n",
    "    numRepeats = len(noiseRange)\n",
    "\n",
    "    # Compute the automutual information across a range of noise levels\n",
    "    amis = np.zeros(numRepeats)\n",
    "    if amiMethod in ['std1', 'std2', 'quantiles', 'even']:\n",
    "        # histogram-based methods using my naive implementation in CO_Histogram\n",
    "        for i in range(numRepeats):\n",
    "            # use default num of bins for CO_HistogramAMI if not specified\n",
    "            amis[i] = CO_HistogramAMI(y + noiseRange[i]*noise, tau, amiMethod, extraParam or 10)\n",
    "            if np.isnan(amis[i]):\n",
    "                raise ValueError('Error computing AMI: Time series too short (?)')\n",
    "    if amiMethod in ['gaussian','kernel','kraskov1','kraskov2']:\n",
    "        for i in range(numRepeats):\n",
    "            amis[i] = IN_AutoMutualInfo(y + noiseRange[i]*noise, tau, amiMethod, extraParam)\n",
    "            if np.isnan(amis[i]):\n",
    "                raise ValueError('Error computing AMI: Time series too short (?)')\n",
    "    \n",
    "    # Output statistics\n",
    "    out = {}\n",
    "    # Proportion decreases\n",
    "    out['pdec'] = np.sum(np.diff(amis) < 0) / (numRepeats - 1)\n",
    "\n",
    "    # Mean change in AMI\n",
    "    out['meanch'] = np.mean(np.diff(amis))\n",
    "\n",
    "    # Autocorrelation of AMIs\n",
    "    out['ac1'] = CO_AutoCorr(amis, 1, 'Fourier')[0]\n",
    "    out['ac2'] = CO_AutoCorr(amis, 2, 'Fourier')[0]\n",
    "\n",
    "    # Noise level required to reduce ami to proportion x of its initial value\n",
    "    firstUnderVals = [0.75, 0.50, 0.25]\n",
    "    for val in firstUnderVals:\n",
    "        out[f'firstUnder{val*100}'] = firstUnder_fn(val * amis[0], noiseRange, amis)\n",
    "\n",
    "    # AMI at actual noise levels: 0.5, 1, 1.5 and 2\n",
    "    noiseLevels = [0.5, 1, 1.5, 2]\n",
    "    for nlvl in noiseLevels:\n",
    "        out[f'ami_at_{int(nlvl*10)}'] = amis[np.argmax(noiseRange >= nlvl)]\n",
    "\n",
    "    # Count number of times the AMI function crosses its mean\n",
    "    out['pcrossmean'] = np.sum(np.diff(np.sign(amis - np.mean(amis))) != 0) / (numRepeats - 1)\n",
    "\n",
    "    # Fit exponential decay\n",
    "    expFunc = lambda x, a, b : a * np.exp(b * x)\n",
    "    popt, pcov = curve_fit(expFunc, noiseRange, amis, p0=[amis[0], -1])\n",
    "    out['fitexpa'], out['fitexpb'] = popt\n",
    "    residuals = amis - expFunc(noiseRange, *popt)\n",
    "    ss_res = np.sum(residuals**2)\n",
    "    ss_tot = np.sum((amis - np.mean(amis))**2)\n",
    "    out['fitexpr2'] = 1 - (ss_res / ss_tot)\n",
    "    out['fitexpadjr2'] = 1 - (1-out['fitexpr2'])*(len(amis)-1)/(len(amis)-2-1)\n",
    "    out['fitexprmse'] = np.sqrt(np.mean(residuals**2))\n",
    "\n",
    "    # Fit linear function\n",
    "    p = np.polyfit(noiseRange, amis, 1)\n",
    "    out['fitlina'], out['fitlinb'] = p\n",
    "    lin_fit = np.polyval(p, noiseRange)\n",
    "    out['linfit_mse'] = np.mean((lin_fit - amis)**2)\n",
    "\n",
    "    return out\n",
    "\n",
    "# helper functions\n",
    "def firstUnder_fn(x, m, p):\n",
    "    \"\"\"\n",
    "    Find the value of m for the first time p goes under the threshold, x. \n",
    "    p and m vectors of the same length\n",
    "    \"\"\"\n",
    "    first_i = next((m_val for m_val, p_val in zip(m, p) if p_val < x), m[-1])\n",
    "    return first_i\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

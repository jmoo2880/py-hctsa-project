{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nitime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1 = np.loadtxt(\"ts1.txt\")\n",
    "ts2 = np.loadtxt(\"ts2.txt\")\n",
    "ts3 = np.loadtxt(\"ts3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Operations.CO_FirstCrossing import CO_FirstCrossing\n",
    "\n",
    "def CO_glscf(y, alpha, beta, tau = 'tau'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Set tau to first zero-crossing of the autocorrelation function with the input 'tau'\n",
    "    if tau == 'tau':\n",
    "        tau = CO_FirstCrossing(y, 'ac', 0, 'discrete')\n",
    "    \n",
    "    # Take magnitudes of time-delayed versions of the time series\n",
    "    y1 = np.abs(y[:-tau])\n",
    "    y2 = np.abs(y[tau:])\n",
    "\n",
    "\n",
    "    p1 = np.mean(np.multiply((y1 ** alpha), (y2 ** beta)))\n",
    "    p2 = np.multiply(np.mean(y1 ** alpha), np.mean(y2 ** beta))\n",
    "    p3 = np.sqrt(np.mean(y1 ** (2*alpha)) - (np.mean(y1 ** alpha))**2)\n",
    "    p4 = np.sqrt(np.mean(y2 ** (2*beta)) - (np.mean(y2 ** beta))**2)\n",
    "\n",
    "    glscf = (p1 - p2) / (p3 * p4)\n",
    "\n",
    "    return glscf    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25284356429315435"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CO_glscf(ts1, 0.9, 0.4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Operations.CO_FirstCrossing import CO_FirstCrossing\n",
    "from Operations.CO_glscf import CO_glscf\n",
    "\n",
    "def CO_fzcglscf(y, alpha, beta, maxtau = None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    N = len(y) # the length of the time series\n",
    "\n",
    "    if maxtau is None:\n",
    "        maxtau = N\n",
    "    \n",
    "    glscfs = np.zeros(maxtau)\n",
    "\n",
    "    for i in range(1, maxtau+1):\n",
    "        tau = i\n",
    "\n",
    "        glscfs[i-1] = CO_glscf(y, alpha, beta, tau)\n",
    "        if (i > 1) and (glscfs[i-1]*glscfs[i-2] < 0):\n",
    "            # Draw a straight line between these two and look at where it hits zero\n",
    "            out = i - 1 + glscfs[i-1]/(glscfs[i-1]-glscfs[i-2])\n",
    "            return out\n",
    "    \n",
    "    return maxtau # if the function hasn't exited yet, set output to maxtau \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "def DN_cv(x, k = 1):\n",
    "    \"\"\"\n",
    "    Coefficient of variation\n",
    "\n",
    "    Coefficient of variation of order k is sigma^k / mu^k (for sigma, standard\n",
    "    deviation and mu, mean) of a data vector, x\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    x (array-like): The input data vector\n",
    "    k (int, optional): The order of coefficient of variation (k = 1 is default)\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float: The coefficient of variation of order k\n",
    "    \"\"\"\n",
    "    if not isinstance(k, int) or k < 0:\n",
    "        warnings.warn('k should probably be a positive integer')\n",
    "        # Carry on with just this warning, though\n",
    "    \n",
    "    # Compute the coefficient of variation (of order k) of the data\n",
    "    return (np.std(x, ddof=1) ** k) / (np.mean(x) ** k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.269342687595913"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_cv(ts3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2765413621752804"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_nlogL_norm(ts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def DN_HighLowMu(y):\n",
    "    \"\"\"\n",
    "    The highlowmu statistic.\n",
    "\n",
    "    The highlowmu statistic is the ratio of the mean of the data that is above the\n",
    "    (global) mean compared to the mean of the data that is below the global mean.\n",
    "\n",
    "    Paramters:\n",
    "    ----------\n",
    "    y (array-like): The input data vector\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float: The highlowmu statistic.\n",
    "    \"\"\"\n",
    "    mu = np.mean(y) # mean of data\n",
    "    mhi = np.mean(y[y > mu]) # mean of data above the mean\n",
    "    mlo = np.mean(y[y < mu]) # mean of data below the mean\n",
    "    out = np.divide((mhi-mu), (mu-mlo)) # ratio of the differences\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0325203252032518"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_HighLowMu(ts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def BF_SimpleBinner(xData, numBins):\n",
    "    \"\"\"\n",
    "    Generate a histogram from equally spaced bins.\n",
    "   \n",
    "    Parameters:\n",
    "    xData (array-like): A data vector.\n",
    "    numBins (int): The number of bins.\n",
    "\n",
    "    Returns:\n",
    "    tuple: (N, binEdges)\n",
    "        N (numpy.ndarray): The counts\n",
    "        binEdges (numpy.ndarray): The extremities of the bins.\n",
    "    \"\"\"\n",
    "    minX = np.min(xData)\n",
    "    maxX = np.max(xData)\n",
    "    \n",
    "    # Linearly spaced bins:\n",
    "    binEdges = np.linspace(minX, maxX, numBins + 1)\n",
    "    N = np.zeros(numBins, dtype=int)\n",
    "    \n",
    "    for i in range(numBins):\n",
    "        if i < numBins - 1:\n",
    "            N[i] = np.sum((xData >= binEdges[i]) & (xData < binEdges[i+1]))\n",
    "        else:\n",
    "            # the final bin\n",
    "            N[i] = np.sum((xData >= binEdges[i]) & (xData <= binEdges[i+1]))\n",
    "    \n",
    "    return N, binEdges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def DN_Cumulants(y, cumWhatMay = 'skew1'):\n",
    "    \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    if cumWhatMay == 'skew1':\n",
    "        out = stats.skew(y)\n",
    "    elif cumWhatMay == 'skew2':\n",
    "        out = stats.skew(y, bias=False)\n",
    "    elif cumWhatMay == 'kurt1':\n",
    "        out = stats.kurtosis(y, fisher=False)\n",
    "    elif cumWhatMay == 'kurt2':\n",
    "        out = stats.kurtosis(y, bias=False, fisher=False)\n",
    "    else:\n",
    "        raise ValueError('Requested Unknown cumulant must be: skew1, skew2, kurt1, or kurt2')\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.497362974297833"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_Cumulants(ts1, cumWhatMay='kurt1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4958467355321061"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DN_Cumulants(ts1, cumWhatMay='kurt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def EN_ApEN(y, mnom = 1, rth = 0.2):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    r = rth * np.std(y, ddof=1) # threshold of similarity\n",
    "    N = len(y) # time series length\n",
    "    phi = np.zeros(2) # phi[0] = phi_m, phi[1] = phi_{m+1}\n",
    "\n",
    "    for k in range(2):\n",
    "        m = mnom+k # pattern length\n",
    "        C = np.zeros(N - m + 1)\n",
    "        # define the matrix x, containing subsequences of u\n",
    "        x = np.zeros((N-m+1, m))\n",
    "\n",
    "        # Form vector sequences x from the time series y\n",
    "        x = np.array([y[i:i+m] for i in range(N - m + 1)])\n",
    "        \n",
    "        for i in range(N - m + 1):\n",
    "            # Calculate the number of x[j] within r of x[i]\n",
    "            d = np.abs(x - x[i])\n",
    "            if m > 1:\n",
    "                d = np.max(d, axis=1)\n",
    "            C[i] = np.sum(d <= r) / (N - m + 1)\n",
    "\n",
    "        phi[k] = np.mean(np.log(C))\n",
    "\n",
    "    return phi[0] - phi[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.514020217676181"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EN_ApEN(ts3, 2, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.mlab\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import math\n",
    "import matplotlib\n",
    "\n",
    "def MD_hrv_classic(y):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Standard defaults\n",
    "    y = np.array(y)\n",
    "    diffy = np.diff(y)\n",
    "    N = len(y)\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Calculate pNNx percentage\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # pNNx: recommendation as per Mietus et. al. 2002, \"The pNNx files: ...\", Heart\n",
    "    # strange to do this for a z-scored time series...\n",
    "    Dy = np.abs(diffy)\n",
    "    PNNxfn = lambda x : np.mean(Dy > x/1000)\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    out['pnn5'] = PNNxfn(5) # 0.0055*sigma\n",
    "    out['pnn10'] = PNNxfn(10) # 0.01*sigma\n",
    "    out['pnn20'] = PNNxfn(20) # 0.02*sigma\n",
    "    out['pnn30'] = PNNxfn(30) # 0.03*sigma\n",
    "    out['pnn40'] = PNNxfn(40) # 0.04*sigma\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Calculate PSD\n",
    "    # ------------------------------------------------------------------------------\n",
    "\n",
    "    F, Pxx = signal.periodogram(y, window=np.hanning(N))\n",
    "\n",
    "    # Calculate spectral measures such as subband spectral power percentage, LF/HF ratio etc.\n",
    "    LF_lo = 0.04 # /pi -- fraction of total power (max F is pi)\n",
    "    LF_hi = 0.15\n",
    "    HF_lo = 0.15\n",
    "    HF_hi = 0.4\n",
    "\n",
    "    fbinsize = F[1] - F[0]\n",
    "\n",
    "    # Calculate PSD\n",
    "    f, Pxx = signal.periodogram(y, window='hann', detrend=False)\n",
    "    f *= 2 * np.pi\n",
    "    #print(Pxx)\n",
    "\n",
    "    # Calculate spectral measures\n",
    "    LF_lo, LF_hi = 0.04, 0.15\n",
    "    HF_lo, HF_hi = 0.15, 0.4\n",
    "\n",
    "    fbinsize = f[1] - f[0]\n",
    "    indl = (f >= LF_lo) & (f <= LF_hi)\n",
    "    indh = (f >= HF_lo) & (f <= HF_hi)\n",
    "    indv = f <= LF_lo\n",
    "\n",
    "    lfp = fbinsize * np.sum(Pxx[indl])\n",
    "    hfp = fbinsize * np.sum(Pxx[indh])\n",
    "    vlfp = fbinsize * np.sum(Pxx[indv])\n",
    "    total = fbinsize * np.sum(Pxx)\n",
    "\n",
    "    out['lfhf'] = lfp / hfp\n",
    "    out['vlf'] = vlfp / total * 100\n",
    "    out['lf'] = lfp / total * 100\n",
    "    out['hf'] = hfp / total * 100\n",
    "\n",
    "    # Triangular histogram index\n",
    "    numBins = 10\n",
    "    hist = np.histogram(y, bins=numBins)\n",
    "    out['tri'] = len(y)/np.max(hist[0])\n",
    "\n",
    "    # Poincare plot measures:\n",
    "    # cf. \"Do Existing Measures ... \", Brennan et. al. (2001), IEEE Trans Biomed Eng 48(11)\n",
    "    rmssd = np.std(diffy, ddof=1)\n",
    "    sigma = np.std(y, ddof=1)\n",
    "\n",
    "    out[\"SD1\"] = 1/math.sqrt(2) * rmssd * 1000\n",
    "    out[\"SD2\"] = math.sqrt(2 * sigma**2 - (1/2) * rmssd**2) * 1000\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pnn5': 0.982982982982983,\n",
       " 'pnn10': 0.9669669669669669,\n",
       " 'pnn20': 0.9359359359359359,\n",
       " 'pnn30': 0.9029029029029029,\n",
       " 'pnn40': 0.8708708708708709,\n",
       " 'lfhf': 8.4832685576491e-08,\n",
       " 'vlf': 1.1046723335508328e-08,\n",
       " 'lf': 8.483267835924858e-06,\n",
       " 'hf': 99.99999149238012,\n",
       " 'tri': 4.830917874396135,\n",
       " 'SD1': 99.72437694213063,\n",
       " 'SD2': 996.9513129028788}"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MD_hrv_classic(ts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def BF_MakeBuffer(y, bufferSize):\n",
    "    \"\"\"\n",
    "    Make a buffered version of a time series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array-like\n",
    "        The input time series.\n",
    "    buffer_size : int\n",
    "        The length of each buffer segment.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_buffer : ndarray\n",
    "        2D array where each row is a segment of length `buffer_size` \n",
    "        corresponding to consecutive, non-overlapping segments of the input time series.\n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "\n",
    "    numBuffers = int(np.floor(N/bufferSize))\n",
    "\n",
    "    # may need trimming\n",
    "    y_buffer = y[:numBuffers*bufferSize]\n",
    "    # then reshape\n",
    "    y_buffer = y_buffer.reshape((numBuffers,bufferSize))\n",
    "\n",
    "    return y_buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def EN_wentropy(y, whaten = 'shannon', p = None):\n",
    "    \"\"\"\n",
    "    Entropy of time series using wavelets.\n",
    "    Uses a python port of the MATLAB wavelet toolbox wentropy function.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    y : array_like\n",
    "        Input time series\n",
    "    whaten : str, optional\n",
    "        The entropy type:\n",
    "        - 'shannon' (default)\n",
    "        - 'logenergy'\n",
    "        - 'threshold' (with a given threshold)\n",
    "        - 'sure' (with a given parameter)\n",
    "        (see the wentropy documentation for information)\n",
    "    p : any, optional\n",
    "        the additional parameter needed for threshold and sure entropies\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    out : float\n",
    "        Entropy value. \n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "\n",
    "    if whaten == 'shannon':\n",
    "        # compute Shannon entropy\n",
    "        out = wentropy(y, 'shannon')/N\n",
    "    elif whaten == 'logenergy':\n",
    "        out = wentropy(y, 'logenergy')/N\n",
    "    elif whaten == 'threshold':\n",
    "        # check that p has been provided\n",
    "        if p is not None:\n",
    "            out = wentropy(y, 'threshold', p)/N\n",
    "        else:\n",
    "            raise ValueError(\"threshold requires an additional parameter, p.\")\n",
    "    elif whaten == 'sure':\n",
    "        if p is not None:\n",
    "            out = wentropy(y, 'sure', p)/N\n",
    "        else:\n",
    "            raise ValueError(\"sure requires an additional parameter, p.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown entropy type {whaten}\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# helper functions\n",
    "# taken from https://github.com/fairscape/hctsa-py/blob/master/PeripheryFunctions/wentropy.py\n",
    "def wentropy(x, entType = 'shannon', additionalParameter = None):\n",
    "\n",
    "    if entType == 'shannon':\n",
    "        x = np.power(x[ x != 0 ],2)\n",
    "        return - np.sum(np.multiply(x,np.log(x)))\n",
    "\n",
    "    elif entType == 'threshold':\n",
    "        if additionalParameter is None or isinstance(additionalParameter, str):\n",
    "            return None\n",
    "        x = np.absolute(x)\n",
    "        return np.sum((x > additionalParameter))\n",
    "\n",
    "    elif entType == 'norm':\n",
    "        if additionalParameter is None or isinstance(additionalParameter,str) or additionalParameter < 1:\n",
    "            return None\n",
    "        x = np.absolute(x)\n",
    "        return np.sum(np.power(x, additionalParameter))\n",
    "\n",
    "    elif entType == 'sure':\n",
    "        if additionalParameter is None or isinstance(additionalParameter,str):\n",
    "            return None\n",
    "\n",
    "        N = len(x)\n",
    "        x2 = np.square(x)\n",
    "        t2 = additionalParameter**2\n",
    "        xgt = np.sum((x2 > t2))\n",
    "        xlt = N - xgt\n",
    "\n",
    "        return N - (2*xlt) + (t2 *xgt) + np.sum(np.multiply(x2,(x2 <= t2)))\n",
    "\n",
    "    elif entType == 'logenergy':\n",
    "        x = np.square(x[x != 0])\n",
    "        return np.sum(np.log(x))\n",
    "\n",
    "    else:\n",
    "        print(\"invalid entropy type\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.22589264096854644"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EN_wentropy(ts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
